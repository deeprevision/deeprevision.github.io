[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Introducing Deep Learning Revision Research Blog",
    "section": "",
    "text": "Greetings,\nI am delighted to introduce Deep Learning Revision research blog. The aim of Deep Learning Revision blog is to elucidate both fundamental principles and cutting-edge techniques in AI research.\nAI research has experienced a Cambrian explosion in recent years. Since 2012 when AlexNet showed significant performance on image recognition, deep learning has profoundly reshaped the field of AI. The period between 2010-2020 was especially ripe with innovations:\n\nDevelopment of new models: The past decade has seen the emergence of revolutionary models such Transformer neural network, which have changed the game by making it possible to learn from vast amounts of data in a way that was not previously possible.\nImproved optimization techniques that have made the model training process more efficient and manageable.\nEnhanced evaluation benchmarks: Evaluation metrics and benchmarks have also seen significant improvements. From standard datasets in image recognition (like ImageNet) to benchmarks in natural language understanding (such as GLUE and SuperGLUE), these advancements have provided the community with standard platforms to test and compare the performance of various models and techniques across a wide range of tasks.\n\nWe have seen models like Transformers taking over natural language processing(NLP), computer vision, and at the same time showing potential in complex problems such as in robotics, reinforncement learning, etc…It’s even unbelievable what happened in the last two years. For instance, we have witnessed AI systems that are capable of generating photorealistic images, transcribing speeches with high accuracy, understanding multiple modalities, and generating realistic texts. Let’s expand that and provide a few specific examples:\n\nImage generation is arguably one of the fields that have had massive breakthroughts in last two years. In 2017, at best, the images you could generate were 32x32 pixels filled up with too much blogs. Fast forward, in 2023, image generation systems have improved to the extent it’s hard to differentiate real and fake images, and in the future, it will be the same for videos as well, if not already. Examples of seminal works in image generation are DALLE•2, Stable Diffusion, Imagen, among others.\nText generation has had many breakthroughs as image generation in recent times. Large language models have taken the world by a storm. The biggest revolution has mostly been in natural language interfaces like ChatGPT and Google Bard. Natural language interfaces are powered by large language models pretrained on massive amount of text data. Example of large language models are GPT-3, GPT-4, PaLM, PaLM-2, LLaMA, LLaMA2, Chinchilla, BLOOM, among others.\nMultimodal learning is another concrete example of recent advancements in deep learning. Designing single systems that can see and hear what’s around and respond accordingly is the holy grail of AI. Although there are still challenges, it is inaruagable that the AI research community has solved independent modalities to a large extent. Agents in real-world however must have the ability to learn from multiple modalities jointly. A challenge now is to design AI systems that can efficiently extract meaningful representations from different modalities without requiring modality-specific encodings. There has been many remarkable works in multimodal learning(most of them are surprisingly visual language models). Notable examples are Flamingo, Gato, BLIP(and BLIP-2), PaLI and PaLI-X, among others. With the advancements of visual recognition models and language models as generalization engine, I expect to see massive breakthroughts in this area in the next months.\nRobotics is another field that is yet to be influenced by deep learning. Robotic tasks typically require low-level engineering and there are so much potential if modern deep learning algorithms can take care of those low-level tasks by learning from massive amount of data. Robotics as a field poses many challenges, but there are many ongoing works around deep robot learning and this also the field that is going to shine in the next few years. Some notable works around deep robotic learning that was published recently are SayCan, Robotic Transformer(RT-1), VIMA, RoboCat, among others.\n\nI see this blog as a little corner in the universe where we can discuss recent research, deconstruct papers, and really try to understand what’s going on. I plan to publish well-studied materials across foundational techniques and some emerging topics. This is new project and as with other new projects, I don’t have everything figured out. New articles will be released irregularily, some may take longer(I tend not to compromise quality for speed), and some other unprojected challenges. All in all, I am super excited for this research blog and I can’t wait to publish the first article in the next few days.\nUntil the first article!\nCheers!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Research Blog",
    "section": "",
    "text": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture\n\n\n\n\n\n\n\ntransformers\n\n\nneural architectures\n\n\nNLP\n\n\ncomputer vision\n\n\ndeep learning\n\n\n\n\nA deep dive into Transformer, a neural network architecture that was introduced in the famous paper “attention is all you need” in 2017, its applications, impacts, challenges and future directions\n\n\n\n\n\n\nJul 29, 2023\n\n\nJean Nyandwi\n\n\n43 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing Deep Learning Revision Research Blog\n\n\n\n\n\n\n\nupdates\n\n\nresearch\n\n\ndeep learning\n\n\n\n\nWelcome to a new research blog!\n\n\n\n\n\n\nJul 23, 2023\n\n\nJean de Dieu Nyandwi\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Deep Learning Revision is an operating manual of AI research. Our mission is to distill and clarify both foundational theories and cutting-edge approaches in the realm of AI research."
  },
  {
    "objectID": "posts/001-transformer/index.html",
    "href": "posts/001-transformer/index.html",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "",
    "text": "Invented in 2017 and first presented in the ground-breaking paper “Attention is All You Need”(Vaswani et al. 2017), the transformer model has been a revolutionary contribution to deep learning and arguably, to computer science as a whole. Born as a tool for neural machine translation, it has proven to be far-reaching, extending its applicability beyond Natural Language Processing (NLP) and cementing its position as a versatile and general-purpose neural network architecture.\nIn this comprehensive guide, we will dissect the transformer model to its core, thoroughly exploring every key component from its attention mechanism to its encoder-decoder structure. Not stopping at the foundational level, we will traverse the landscape of large language models that leverage the power of the transformer, delving into their unique design attributes and functionalities. Further expanding the horizons, we will explore the applications of transformer models beyond NLP and probe into the current challenges and potential future directions of this influential architecture. Additionally, a curated list of open-source implementations and supplementary resources will be provided for those intrigued to explore further.\nWithout bells and whistles, let’s dive in!\n Figure 0: Transformer Architecture that we will explore in depth in this article. Adapted from (Vaswani et al. 2017)."
  },
  {
    "objectID": "posts/001-transformer/index.html#multilayer-perceptronsmlps",
    "href": "posts/001-transformer/index.html#multilayer-perceptronsmlps",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "MultiLayer Perceptrons(MLPs)",
    "text": "MultiLayer Perceptrons(MLPs)\nLet’s start with multilayer perceptrons(MLPs), one of the classical neural network approaches. MLPs are not super powerful themselves but you will find them integrated in almost any other architecture(surprisingly even in transformer). MLPs are basically a sequence of linear layers or fully connected layers.\n Figure 1: Multilayer Perceptrons(MLPs).\nMLPs have long been used to model different kinds of data way before the AI community find best architectures for various modalities but one thing for sure, they are not suitable for sequence modelling. Due to their feedforward design, they can not preserve the order of information in a sequence. Sequence data lose meaning when the order of the data is lost. Thus, the inability of MLPs to preserve order of information make them unsuitable for sequence modelling. Also, MLPs takes lots of paramaters which is another undesired property a neural network can have."
  },
  {
    "objectID": "posts/001-transformer/index.html#convolutional-neural-networks",
    "href": "posts/001-transformer/index.html#convolutional-neural-networks",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Convolutional Neural networks",
    "text": "Convolutional Neural networks\nConvolutional neural networks(CNNs or ConvNets) are a class of neural network architectures that are most known for processing images and other modalities such as texts and videos.\n Figure 2: Convolutional neural networks for text understanding(X. Zhang and LeCun 2015).\nConvNets have so far been successful in small scale and large scale visual recognition but not quite successful in sequence modelling. They are easy to parallize(good for GPUs), due to their locality(computations are bundled in local parts of the input data), they require many layers to handle long-term dependencies. As opposed to images that have fixed length, most sequential data have variable length, something that neither ConvNets or MLPs can handle."
  },
  {
    "objectID": "posts/001-transformer/index.html#recurrent-neural-networks",
    "href": "posts/001-transformer/index.html#recurrent-neural-networks",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\nUnlike MLPs or ConvNets, recurrent neural networks(RNNs) were designed with sequence in mind. RNNs have feedback loop in their design, a key element in their ability to model sequential data. Another desirable property of RNNs is that they can handle variable length data.\nThere are fundamental problems in how RNNs are wired. Firstly, due to their sequential design, they are likely to be unstable for long-term sequences. Secondly, they can not parallized which limit their scalability on modern machine learning accelerators(like GPUs).\n\nFigure 3: Recurrent neural networks(RNNs).\nRecurrent networks have many variations. One of their famous version is Long Short Term Memories(LSTMs). LSTMs can handle long-term sequences. They have a cellstate(horizontal straight line in figure below) and gates which all smooth the flow of information.\n\nFigure 4: Long Short Term Memories(LSTMs).\nAnother slightly efficient version of LSTMs is gate recurrent Units(GRUs). LSTMs works great for basic sequence modelling problems but they are still limited in how far they can go. As we previously said, they can not parallized which means they can not be scaled. Also, even if they can preserve the order of information, they can not reason about the global context of the data they are processing. Context is important. Take an example in machine translation(the task that basically gave us transformer), context of sentence being translated is as important as the order.\nAll we have been doing basically is to motivate the transformers. So far, we have seen that prior neural networks were either not suitable for sequence modelling or not parallizable or not stable or limited in context length, all of which are primary desirable traits of sequence neural architectures.\nNow that we have the right background, let’s dive into transformer architecture."
  },
  {
    "objectID": "posts/001-transformer/index.html#encoder",
    "href": "posts/001-transformer/index.html#encoder",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Encoder",
    "text": "Encoder\nEncoder is one of the main blocks of the transformer architecture that is right at the input of input sequence. Encoder transforms input sequence into compressed representation. In the orginal transformer architecture, the encoder was repeated 6 times(this depends on overall size of architecture, it can be changed). Each encoder block has 3 main layers which are multi-head attention(MHA), layer norm, and MLPs(or feedforward according to the paper).\nMulti-head attention and MLPs are referred to as sub-layers in the transformer paper. Between sublayers, there are layer normalization and dropout and residual connections in between(refer to diagram for correct flow of those layers).\nThe number of encoder layers was 6 as said previously. The more the number of encoder layers, the larger the model, and the more the model is likely to capture the global context of the input sequences hence resulting in better task generalization."
  },
  {
    "objectID": "posts/001-transformer/index.html#decoder",
    "href": "posts/001-transformer/index.html#decoder",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Decoder",
    "text": "Decoder\nThe decoder is pretty much the same as encoder except additional multi-head attention that operated over the output of the encoder. The goal of the decoder is to fuse encoder output with the target sequence and to make predictions(or to predict the next token).\nThe attention that takes the target sequence in decoder is masked to prevent the current token(being processed) from attending to subsquent tokens in the target sequence. If the decoder has access to a full target sequence, this would basically be cheating and can result in model that can not generalize beyond the training data.\nDecoder is also typically repeated the same times as encoder. In the orginal transformer, the number of decoder blocks were also 6 blocks."
  },
  {
    "objectID": "posts/001-transformer/index.html#attention",
    "href": "posts/001-transformer/index.html#attention",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Attention",
    "text": "Attention\n\nWhat Really is Attention?\nAttention is the principal element of transformer architecture. In essence, attention is a mechanism that can allow the neural network to pay more attention to the part of input data that contains meaningful information and pay less attention to the rest of the input.\nThe attention mechanism was used in various tasks long before the introduction of transformer architecture. The idea of attention first appeared in neural machine translation(NMT) approach that used attention to find the set of positions in input sentence where the most relevant information is concentrated(Bahdanau, Cho, and Bengio 2014). Because their attention based NMT could align and translate jointly or simultaneously, it surprisingly performed well than previous approaches. As you can see in the image below, the network was able to find the correct order of words in a translated sentence, a feat that prior neural machine translation approaches struggled to achieve.\n\nFigure 6: Aligning the source sentence and target sentence in neural machine learning translation(Bahdanau, Cho, and Bengio 2014). The x-axis and y-axis shows the source sentence and translated sentence respectively. Each pixels indicates the attention weights of source(input) token with its corresponding target token. The diagonal attention represents words that are in corresponding order(ex: the agreement on the -&gt; L’ accord sur la). Attention can figure out the correct word order(ex: European Economic Area -&gt; zone économique européenne).\nWhat’s going on in the image above? Can you spot something? The order of words was reversed in translated sentence wherever it make sense in target language. Thus, when translating a sentence, attention can give the model the ability to not only translate the sentence correctly, but to also translate it in the right order based on the context of the target language. In brief, attention can identify and preserve the context when translating one language to another.\nAnother earlier work that used attention is found in neural image captioning(Xu et al. 2015). In this work, the authors used ConvNets for features extraction and RNNs with attention mechanism to generate a caption that aligns best with the input image. The image belows(taken from the paper) shows where the model roughly attends to.\n\nFigure 7: Generating caption with neural captioning model. The white regions show where the model is focusing when generating caption A woman is throwing a frisbee in a park”. Image from (Xu et al. 2015).\nOn a global level, integrating attention mechanism in image captioning model helps the model to attend to the meaningful part of the input image when generating a caption.\n\nFigure 8: The model can attend to key objects when generating captions. Image taken from (Xu et al. 2015).\nBoth the examples we used above demonstrate the effectiveness of attention. Attention is really a magic mechanism that allows the neural network to focus on part of input data that contains meaningful information and focus less on rest of the input data.\nNow that we understand attention, let’s look at the inputs of attention function in transformer architecture: querry, keys, and values.\n\n\nAttention Function: Query, Key, Value\nIntuitively, attention is really “focus on most important part of the input data”. Technically speaking, attention measures the similarity between two vectors and return the weighted similarity scores. A standard attention function takes three main inputs which are query, key, and value vectors. Before breaking down the attention function, let’s try to understand what keys, values, and queries mean.\nQuery, keys, and values are terms commonly used in search engines and database systems. To understand those terms, let’s take a simple example.1 Let’s say you are searching papers that are based on attention on ArXiv. The query is ideally what you will put in the search box. Internally, the ArXiv may organize papers by a set of predefined keys. Before ArXiv gives you papers that you asked for, it will compare your query to those predefined set of keys and return papers that best match with query and keys correspondence. Values merely refers to all papers in the database. As a disclaimer, we are using this example to understand the meaning of query, keys, and values in search and database systems context. It’s not an attempt to show how ArXiv system works.\n\nFigure 9: Example demonstrating query, keys, and values in ArXiv paper search system.\nWith such intuitive understanding of query, keys, and values in mind, let’s move to the mathematical representation of the attention function.\n\\[\nAttention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n\\]\nFrom the function above, \\(Q\\), \\(K\\), \\(V\\) are query matrix, key matrix, value matrix respectively. We compute the dot product of query and keys and divide the product by a scaling factor of \\(\\sqrt{d_k}\\). The scaling factor is used to avoid the scenarios where large values of \\(QK^T\\) would result in small gradients. Then, we normalize the dot product into a probability distribution with softmax(this basically give us weighted sum) and by multiplying it with values, we get weighted values.\n\nFigure 10: Graphical representation of dot-product attention. Figure adapted from (Vaswani, 2017).\nThe kind of attention described above is called scaled-dot product attention, a modified dot-product attention(Luong, Pham, and Manning 2015). There are other kinds of attention such as additive attention(Bahdanau, Cho, and Bengio 2014), content-based attention(Graves, Wayne, and Danihelka 2014), location-based attention(Bahdanau, Cho, and Bengio 2014), and general attention(Luong, Pham, and Manning 2015). Each of those attention types can either be applied globally(to the whole input data), hence global attention, or locally(sub-parts of the input data), hence local attention.\nYou may have heard that transformer is parallizable and you may be wondering where it comes from. Transformer parallization comes from attention function. Provided that both query, keys, and values are matrices, attention can be performed in two main matrix multiplies and hence no loops or any recurrent operation involved. Computing attention is resonably faster for GPUs. For bigger models(in order of billions parameters) and massive training data(in order of billion/trillions tokens), attention is can be expensive since it takes quadratic time complexity from the fact that each token attends other tokens.\n\n\n\n\n\n\nNote\n\n\n\nIf the queries, keys, and values are derived from same source, the attention applied to them is called self-attention. If they come from different source, we say cross-attention.\n\n\n\n\nMulti-Head Attention\nWhat we decribed above is a single attention layer. In practice, you typically would not get sound results with just one attention layer. Instead, people tend to compute multiple attention layers in parallel and concatenate the results. In nutshell, that is multi-head attention. Multi-head attention is basically multiple independent attentions computed over linearly projected QKV vectors. In the figure below of multi-head attention, the concatenated attention values are linearly projected to the model dimension.\n\nFigure 11: Multi-Head attention. Figure adapted from (Vaswani, 2017).\nAs explained by the designers of the transformer architecture, computing multiple attentions in parallel allows the model to “jointly attend to information from different representation subspaces at different positions.”“(Vaswani et al. 2017). A surprising thing about multi-head attention is that it doesn’t increase the overall computation cost because the dimension of each head is oneth of number of heads(i.e, heads in base transformer is 8) of the overall model dimension(ie, 512). So, if the dimension of the model(\\(d_{model}\\) in the paper) is 512, the number of heads in multi-head attention are 8, each head is thus \\(512/8=64\\).\nMulti-head attention can be seen as depth-wise separable convolution(Chollet 2017) in ConvNets. Depth-wise separable convolution is a special type of convolution that splits input tensor into multiple channels, operate on each channel independently, concatenate the individual outputs and and feed the results to a pointwise convolution(1x1 convolution which is equivalent to a linear projection)."
  },
  {
    "objectID": "posts/001-transformer/index.html#mlps",
    "href": "posts/001-transformer/index.html#mlps",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "MLPs",
    "text": "MLPs\nMLPs or Multilayer Perceptrons2 are one of the two sublayers in both encoder and decoder. MLPs in the transformer are made of two linear layers with ReLU activation in between and they are applied to each position independently and identically.\n\nFigure 12:Multi-Layer Perceptrons(MLP) in transformer."
  },
  {
    "objectID": "posts/001-transformer/index.html#embeddings-and-positional-encoding-layers",
    "href": "posts/001-transformer/index.html#embeddings-and-positional-encoding-layers",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Embeddings and Positional Encoding Layers",
    "text": "Embeddings and Positional Encoding Layers\nThe transformer architecture incorporates two embedding layers: one at the encoder to handle the input or source sequence, and another at the decoder for handling target or output sequence. These embedding layers convert input or output tokens into dense vectors of a fixed size, essentially mapping each token in a sequence to a specific dense vector. Utilizing embeddings is a standard practice in language modeling due to the semantic depth they provide. With these embedded token vectors, those bearing similar semantic meanings tend to align in the same direction.3.\nThe size of the embeddings in the base transformer is 512(this is the dimension of the whole model). As a side note here, transformer architecture maintains the same dimension across the whole network and it is 512 for base model. This is what referred to as \\(d_{model}\\) previously.\nPositional encodings serve as integral components in the initial stages of both the encoder and decoder within a Transformer model. They are used to preserve the order of tokens in a sequence. One might question the necessity of these positional embeddings. This stems from the inherent permutation invariance of the attention mechanism, whereby modifying the order of tokens does not alter the output weighted values4. Consequently, the attention mechanism, on its own, lacks awareness of the token order. As the transformer architecture does not incorporate any other recurrence methods, positional encodings are introduced to equip the model with positional awareness of the tokens in the sequence. In essence, without positional encodings, a Transformer would indeed exhibit permutation invariance. However, such a design would fall short for tasks where sequence order holds significance, as is the case for most NLP tasks.\nFor encoding positional information in a sequence, the designers of transformer used sinusoidal functions of different frequencies. They also experimented with learned positional embeddings, but it did not make a difference in the results."
  },
  {
    "objectID": "posts/001-transformer/index.html#residual-connections-layer-normalization-and-dropout",
    "href": "posts/001-transformer/index.html#residual-connections-layer-normalization-and-dropout",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Residual Connections, Layer Normalization, and Dropout",
    "text": "Residual Connections, Layer Normalization, and Dropout\nResidual connections are at the heart of neural network design and they are one of the popular ingredients in modern deep learning. Since when deep residual networks proved substantial performance in computer vision(He et al. 2016), residual connections have been used in almost most neural networks not just in vision but in other modalities as well. In fact, it is almost impossible to see a neural network model that does not use residual connections in present times. Residual connections alleviate unstable gradient problems and they help the model to converge faster.\nOne of the transformer authors, Ashish Vaswani once said that “residual connections carry positional information to higher layers, among other information.” Take a look at the image below!\n\nFigure 13: Residual connections carry signals to higher layers which improves the training of transformer model. The smooth diagonal in first image(with residuals) shows the effectiveness of residual connections. Image by Ashish Vaswani in CS224N.\nLayer normalization(Ba, Kiros, and Hinton 2016) is also one of the most used normalization techniques in modern neural networks. Layer normalization significantly reduces the training time by normalizing the activations of a layer with the layer mean and variance. Unlike batch normalization(Ioffe and Szegedy 2015) that normalizes each layer with mean and variance computed over the mini-batch, layer norm just normalizes each layer with the mean and variance of each activation. Layer normalization maintains similar behavior during both training and testing phases, unlike batch normalization which exhibits different behaviors in these two stages.\nThere are two ways to place layer normalization in transformer architecture. The first option is called Post layer normalization(Post-LN) where layer normalization is placed between residual blocks(or after each sublayer(multihead-attention and MLPs) but after addition). The second option is called Pre layer normalization(Pre-LN) where layer normalization is placed before each sublayer inside the residual block. The standard transformer architecture uses Post-LN, but in the updated codebase that trained the orginal transformer5, it was found that to be Pre-LN. This mismatch between paper and codes makes it hard to trace back the actual position of layer normalization in initial transformer but from the commit history, it looks like Pre-LN was used later. The authors could have updated the paper but they probably didn’t mind since no one knew this would turn out to be one of the influential and reference papers in neural network design.\n\nFigure 14: Post layer normalization(Post-LN) and Pre layer normalization(Pre-LN).\nThus, it’s not exactly clear where the layer normalization should be and this is an active research question. A recent study on the impacts of Pre-LN and Post-LN(Xiong et al. 2020) showed that placing layer normalization before multi-head attention and MLPs(Pre-LN) improves the training and converge much faster than layer normization placed after multi-head attention and MLPs. The study also claimed that with Pre-LN, you don’t need to be smart at choosing learning-rate scheduler since Pre-LN have better initializations. Neither of Pre-LN an Post-LN is perfect. Another quite recent study introduced ResDual(Xie et al. 2023) which basically alleviates issues of Pre-LN and Post-LN by introducing additional residual connection with layer normalization.\nWhere you should place layer normalization continue to be a question but this should be less of a question. As many people have noted, transformer seems to be a universal architecture. The orginal vanilla transformer(with few tweaks like yes LN) is the one that is still behind most novel works in language modelling, visual recognition, and multimodal learning depsite millions number of works that claims to improve the transformer. Thus, we should aim to keep the universality of this architecture. We will see this more in efficient transformers toward the end of the article.\nBefore we wrap up this section, let’s talk about dropout(Srivastava et al. 2014) in the transformer architecture. Layer normalization can acts as a regularizer as a side effect but you still need other forms of network regularizations to deal with overfitting. Dropout is applied to the output of each sublayer(before addition and normalization). It is also applied to the sum of the embeddings and the positional encodings in both encoder and decoder stacks. For other regularization techniques used in training transformer and other training details, check out the paper for more."
  },
  {
    "objectID": "posts/001-transformer/index.html#linear-and-softmax-layers",
    "href": "posts/001-transformer/index.html#linear-and-softmax-layers",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Linear and Softmax Layers",
    "text": "Linear and Softmax Layers\nThe linear layer after decoder takes the decoded activations and project them to the size of the vocabulary. This linear layer will basically produce logits. The softmax layer will take those logits and turn them into next-token probabilities. The next predicted token will be basically the argmax of softmax output."
  },
  {
    "objectID": "posts/001-transformer/index.html#evolution-of-llms",
    "href": "posts/001-transformer/index.html#evolution-of-llms",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Evolution of LLMs",
    "text": "Evolution of LLMs\nLarge Language Models (LLMs) have revolutionized human interaction with machine learning systems. Natural language interfaces, such as ChatGPT and Bard, are powered by robust LLMs. These models have paved the way for executing natural language downstream tasks on-fly or through zero-shot learning. Such tasks, in the past, necessitated the gathering of a downstream or task-specific datasets.\nAt the core of these LLMs, it’s fundamentaly a transformer model that we have seen with little tweaks here and there. In this section, we will delve into the compressed evolution of Large Language Models. Moreover, we will explore the development of vertical LLMs, specifically designed and fine-tuned for particular applications.\nTransformer base model had 65M parameters but since then, language models got bigger and bigger(in order of billions) and hence the name large language models. Below is a quick overview of popular large language models.\n\nFigure 17: Overview of popular LLMs. Layers are number of stacked encoders/decoders or both for encoder-decoder models, width is the dimension of the model, heads are number of attention layers in multi-head attention, params are number of parameters. N.B, the numbers of heads in GPT-2 are not exactly known.\nThe training process for most large language models (LLMs) follows a broadly similar pattern. In the initial pretraining phase, LLMs are exposed to vast volumes of curated textual data, sourced from a diverse range of materials such as books, articles, code snippets, and websites. This vast dataset is essential for the models to gain a comprehensive understanding of the world, enabling them to create rich representations and generate contextually relevant responses. The general public holds high expectations for LLMs’ performance across various domains. To meet these expectations, the pretraining data must encompass a wide spectrum of topics and disciplines(J. Yang et al. 2023).\nThe actual training of LLMs occurs in an unsupervised fashion, with a specific focus on self-supervised learning(SSL). This approach eliminates the need for labelled data, a crucial feature considering the near-impossibility of labeling the entirety of online content.\n\nFigure 18: A typical training workflow of large language models. LLMs are typically trained on large unlabelled dataset. After, they can be used directly via prompt engineering or they can be fine-tuned further on specialized tasks.\nHowever, training models on unlabelled data requires the clever implementation of training objectives since there is no ground truth for reference. Most LLMs, therefore, utilize the next-token prediction (NTP) as a common training objective. In essence, the LLMs are taught to accurately predict the next token in a sequence, gradually enhancing their understanding and generating capabilities. Another commonly used training objective is masked language modelling(MLM). Masked language models are trained to predict a masked token in a sequence. This objective was popularized by BERT(Devlin et al. 2019).\nAfter pretraining phase, the models can be used to generate texts via techniques like zero-shot learning or few-shots learning. In zero-shot learning, a model is prompted to perform a task(or answer a given question) without any demontrations of how the task is done. In few-shots learning, a model is given a number of demonstrations of how the task is done before it can be asked to perform that task. Zero-shot learning and few-shot learning are examples of in-context learning. In-context learning(ICL) refers to the ability of LLMs to generate coherent texts using semantic prior knowledge(Jerry Wei et al. 2023) and without any parameter updates(Akyürek et al. 2023). Prompting large language models(also known as prompt engineering) is a relatively new field itself and there are other prompt engineering techniques such as chain of thoughts(CoT)(Jason Wei, Nye, and Liang 2022).\nIn-context learning tends to excel at tasks that are considered simple but falls short for tasks that can not be described easily in prompts. Complex tasks requires more than clever prompts. In the words of Karpathy, “reaching top tier performance(on complex tasks) will include finetuning, especially in applications with concrete well-defined tasks where it is possible to collect a lot of data and”practice” on it.”7. Thus, for LLMs to get good performance on specialized tasks like mathematics, medicine, scientific fields(like chemistry), people typically finetune base LLMs on downstream datasets. We will see examples of this in the section of vertical LLMs.\nNow that we’ve briefly introduced Large Language Models (LLMs), it’s time to examine some of the most popular LLMs, focusing specifically on their design choices: whether they function as encoders, decoders, or employ a combined encoder-decoder architecture."
  },
  {
    "objectID": "posts/001-transformer/index.html#encoder-decoder-encoder-decoder-llms",
    "href": "posts/001-transformer/index.html#encoder-decoder-encoder-decoder-llms",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Encoder, Decoder, Encoder-decoder LLMs",
    "text": "Encoder, Decoder, Encoder-decoder LLMs\nThe standard transformer model has encoder-decoder and this has to do with the task it was meant to perform which is machine translation where you have to process both input sentence and its target translation. Since the transformer, AI research community came up with different variations of the architecture for different tasks. Depending on the task, some transformer models maintained encoder-decoder structure, some used decoder only or encoder only. Let’s start with the latter.\n\nEncoder-only LLMs\nEncoder-only LLMs use the encoder part of the standard transformer model. Encoder-only LLMs are typically used for NLP discriminative tasks such as text classification and sentiment analysis.\nBERT(Devlin et al. 2019) is one of most popular encoder-only language models. BERT is one of the earliest works that showed that you can pretrain a transformer(encoder) on large unlabeled text dataset and finetune the same architecture on various downstream tasks with additional task-specific head. The pretraining objectives for BERT were masked language modelling(MLM) and next sentence prediction(NSP)8. With masked language modeling, we mask a given percentage(15% as noted in the paper) of input tokens and the goal is to predict the masked tokens. In next sentence prediction, for two sentences pair making up the input sequence, the goal is to predict whether or not two sentences are in a correct order at random.\n\nFigure 19: Masked language modelling(MLM) in BERT. In the sentence example shown in the figure, the objective of training BERT is to predict the masked word “network”. In next sentence prediction objective, the workflow is roughly the same but instead of predicting the masked tokens, we predict if two sentence pairs separated by SEP token are in correct order.\nBERT is a truly revolutionary technique that improved SOTA on ubiquitous number of NLP downstream tasks. It also inspired other efficient bidirectional architectures for NLP pretraining such as RoBERTa(Y. Liu et al. 2019) standing for Robustly optimized BERT approach. One of the main design choices that RoBERTa introduces is not using next sentence prediction objective.\n\n\nDecoder-only LLMs\nDecoder-only LLMs are based on the decoder part of standard transformer. In transformer architecture, decoder is highly similar to encoder except that the self-attention in decoder is masked to prevent the model to look at subsequent tokens when generating current token.\nDecoder LLMs are trained with next token prediction objective9. As a result, they can only generate one token at time or autoregressively. Overally, decoder models are used in generative tasks.\nThe most popular decoder models are GPT(Generative Pretrained Transformer) models family, most notably GPT-3(Brown et al. 2020) and GPT-4(OpenAI 2023). GPT-3 and GPT-4 are direct scale-up of the early GPT model(Radford et al. 2018). As any other large language model, GPT models are trained on massive amount of unlabelled data(in order of billions to trillions tokens). Due to the large-scale pretraining and suitable training objective, GPT models develops impressive in-context learning capabilities where they can perform a range of NLP downstream tasks without gradient updates or task-specific fine-tuning(Brown et al. 2020). In fact, GPT models can perform tasks like text classification, summarization, question answering on-fly by just prompting the model in zero-shot or few-shot settings10. This remarkable feat of in-context learning has often been called “emergent abilities” of large language models(Jason Wei et al. 2022).\nGPT models are not the only models based on decoder. In fact, most famous LLMs are decoders. Examples include PaLM(Chowdhery et al. 2022), BLOOM(Le Scao et al. 2022), Chinchilla(Hoffmann et al. 2022), LLaMA(Touvron et al. 2023), and many others.\n\n\nEncoder-Decoder LLMs\nEncoder-decoder LLMs looks like the standard transformer. They are generally used in tasks that demands processing two sequences(i.e, input and target are both sequences) such as machine translation. Encoder-decoder style is not widely used compared to other model styles we have seen. The most famous models of this kind are T5(Raffel et al. 2019), BART(Lewis et al. 2019), UL2(Tay et al. 2022), FlanT5(Chung et al. 2022), mT5(Xue et al. 2021), etc…\nEncoder-decoder style is also used in multimodal learning, most notably vision-language pretraining(VLP). Works like SimVLM(Z. Wang et al. 2021) and PaLI-X(X. Chen et al. 2023) employs encoder for learning joint image and text representations and decoder for generating the output."
  },
  {
    "objectID": "posts/001-transformer/index.html#vertical-llms",
    "href": "posts/001-transformer/index.html#vertical-llms",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Vertical LLMs",
    "text": "Vertical LLMs\nMost of LLMs that we outlined above are typically referred to as foundational or frontier LLMs. Foundational models are typically trained on massive amount of data with self-supervision and they can be fine-tuned to a wide range of downstream tasks(Bommasani et al. 2022).\nVertical LLMs are a class of LLMs that are adapted to specific applications. Foundational LLMs can generalize to simple tasks like sentiment analysis but they don’t perform well on complex tasks or tasks that require a domain expertize. For example, a foundational LLM is unlikely to perform well on medical question answering task because it doesn’t have expertize in medicine. More examples: a foundational LLM is unlikely to perform well on legal question answering task because it doesn’t have expertize in law. This is also true in other fields such as finance, physics, chemistry, etc…Vertical LLMs are designed to address this issue. They are trained on a specific domain and they can perform well on tasks that require expertize in that domain. Foundational models aim to be generalists but most of the time, we care about models that can do one thing very well.\nExamples of recent vertical LLMs include MedPaLM(Singhal et al. 2022) and Med-PaLM 2, ClinicalGPT(G. Wang et al. 2023), FinGPT(H. Yang, Liu, and Wang 2023), BloombergGPT(Wu et al. 2023), Galactica(Taylor et al. 2022), Minerva(Lewkowycz et al. 2022), among others.\n\nFigure 20: LLMs Topologies. Adapted from (J. Yang et al. 2023)."
  },
  {
    "objectID": "posts/001-transformer/index.html#efficient-transformers",
    "href": "posts/001-transformer/index.html#efficient-transformers",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Efficient Transformers",
    "text": "Efficient Transformers\nTransformer has shown significant performance across various modalities such as language, vision, robotics, and reinforcement learning. Transformer neural network architecture has a set of traits that make it a suitable architecture for those domains: it is expressive, plays well with current optimization techniques, and it can be parallized. From those traits, one can say that transformer is an efficient architecture. That said however, the efficiency of transformer comes with enormous computatation cost due to the quadratic time and memory complexity of self-attention. The compute requirements of transformer has limited its scalability and its applications in low-budget devices such as smartphones and microcontrollers.\nModel efficiency is an important thing to take into account when developing and deploying machine learning systems because how a model perform during inference can affects user experience(Dehghani et al. 2022). There has been zillion transformer models that claim to improve the efficiency(memory footprint and computational cost) of transformer architecture(those models are typically called “xformers”) but those models usually tend to be targeted at one particular benchmark or device. Most of the new xformers models that claim to reduce the quadratic time and memory complexity of self-attention are much slower than vanilla transformer and they are rarely used in practice and they don’t have the universality of original transformer(Tay et al. 2020).\nAs (Tay et al. 2020) puts it nicely in a survey of “Efficient Transformers”, the ideal xformer should yes reduce the quadratic time complexity of self-attention, but should stay universal and perform well across all tasks and modalities. It should also not trade-off speed for memory, should not be hard-engineered, should stay elegant and simple. For more, I recommend you read the survey paper of efficient transformers.\n Figure 21: A taxonomy of efficient transformers. Image from (Tay et al. 2020) .\nVirtually all modified transformer models compute the approximation of attention to reduce the cost down. As opposed to those approaches, there is actually one kind of attention that computes exact standard attention values but way faster. That approach is FlashAttention(Dao et al. 2022) and we will talk about it on a high-level.\nFlashAttention is fast and memory-efficient algorithm that computes the exact attention. FlashAttention is 2-4x faster than standard attention. It achieves this enormous increase in compute efficiency by using two main techniques: tiling and recomputation. Tiling happens in forward pass and it involves splitting large matrices in attention(K key and V value) into blocks. Rather than computing attention over entire matrices, FlashAttention computes it over blocks and concatenate the resulting blocks saving a huge amount of memory. Recomputation happens in backward pass and it basically means recomputing the attention matrix rather than storing it in forward. The idea of FlashAttention boils down to improving the memory and not decreasing computations because modern GPUs have high theorical FLOPs(Floaping Point Operations, means you want to max that out) but limited memory12(means any saving in memory can improve the training speed). HBM(High Bandwidth Memory) is typically large but it is not faster than on-chip SRAM(Static Random Access Memory) and thus, the computations over blocks(of K and V) happens in SRAM(because it is faster) but all full matrices are stored in HBM(because it’s big). This high-level explanation is probably oversimplication provided that FlashAttention is implemented at the GPU level(with CUDA software) and this is in fact the reason why it is IO aware but hopefully that explains what’s going on in this fast algorithm.\nBelow image shows the memory hierarchy in GPU, FlashAttention algorithm, and amount of time(in ms) taken by each intermediate step in GPT-2 attention versus FlashAttention. Ideally, we would want the bulk of computations to be taken by matrix multiplication(matmul) operations but surprisingly, dropout, softmax, and mask(i.e, GPT-2 is decoder model) end up taking the whole runtime in GPT-2 attention because they are computed over full matrices. Matmuls take less runtime than those other operations because GPUs are exactly designed to be fast at matrix multiplications(they have really high theorical FLOPs and maximizing FLOPs usage doesn’t reduce the runtime). By using tiling and recomputation techniques, the compute time of FlashAttention is significantly low compared to standard attention as you can see below.\n\nFigure 22: The memory hierachy in GPU, FlashAttention algorithm, and runtime of GPT-2 attention vs FlashAttention.\nFlashAttention is intergrated in PyTorch 2.0, Hugging Face transformers, Microsoft’s DeepSpeed, MosaicML composer library and many other library. You can learn more FlashAttention in the paper, or watch this video by core author, and the release blogpost. At the time of writing this section, FlashAttention2(Dao 2023) was also released and it is even faster than FlashAttention version 1 on several orders of magnitude. FlashAttention-2 improves parallelism by parallelizing over sequence length dimension instead of batch size and number of attention heads and splits Q(query) matrix instead of K and V. This release blog post explains well what FlashAttention2 brings to the tensor table."
  },
  {
    "objectID": "posts/001-transformer/index.html#transformers-with-effective-long-contexts",
    "href": "posts/001-transformer/index.html#transformers-with-effective-long-contexts",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Transformers with Effective Long Contexts",
    "text": "Transformers with Effective Long Contexts\nHandling long context length is one of the main active areas of research in Transformer large models. As direct consequence of the quadratic time and memory complexity of attention, transformer fails to process long context windows. Researching techniques that extend the context window of transformer architecture is an important thing since context window determines the amount of information that you can fit in transformer memory during inference. Tasks like long conversations, summarizing long documents, and executing long-term planning may require models that support long context windows(S. Chen et al. 2023).\nAlot have been written about context windows and extending them such as (S. Sun et al. 2021), but I want to highlight a recent paper that presents remarkable findings around long contexts. Recent language models(based on transformer) can take longer contexts but it’s not clear whether long context actually helps. As shown by (N. F. Liu et al. 2023), the performance of language models degrades with increase in input context length. So, even for models that have extended context length, their performance still degrades for longer input contexts. Also, the work also found that language models perform well when the relevant information are placed at the beginning or the end of the input context and significantly degrades when the relevant information are placed in the middle, suggesting that language models are U-shaped reasoners.\nThe findings highlighted above are appealing and provide broad implications that could be applicable in the design of fine-tuning datasets and during in-context learning, but it’s important to note that none of those is established understandings provided that “how transformer models perform on long context windows” is an active area of research. We hope that future transformer models will be able to operate over long input sequences and at the same time perform well regardless of relevant information are placed. This is in fact the holy grail of large language models.\n Figure 23: Language models(based on transformer) tends to perform well when relevant information are at the beginning or at the end of input context(graph on the left) and their performance decreases for longer contexts(graph on the right). The graphs are taken from (N. F. Liu et al. 2023)."
  },
  {
    "objectID": "posts/001-transformer/index.html#multimodal-transformer",
    "href": "posts/001-transformer/index.html#multimodal-transformer",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Multimodal Transformer",
    "text": "Multimodal Transformer\nA primary objective in neural network design is to architect a single, universal model that can efficiently process multiple modalities without necessitating modality-specific encoders or preprocessing. Indeed, transformer models have seen widespread application across various domains, spanning text, images, robotics, and speech. Yet, the goal of creating a truly universal transformer — one that performs equally effectively across all modalities without requiring specific adjustments — remains a challenge. This challenge arises from the inherent differences and complexities in data types and the transformer model itself, which frequently demand modality-specific modifications.\nFor instance, the process for handling text, images, and speech each have unique considerations due to their individual characteristics. Transformers excel in scenarios where data can be framed as a sequence of tokens, however, the method of transposing a particular modality into such a sequence significantly varies among different modalities. Consequently, the challenge lies in designing a singular architecture that can uniformly extract valuable insights from all data types with comparable efficiency.\nThe achievement of such an architecture would signify a monumental stride in the field of multimodal learning, paving the way for models that can seamlessly transition between different types of data and potentially unlocking new avenues of exploration in multimodal representation learning.\nNearly all current state-of-the-arts in multimodal learning typically uses separate tokenizer and encoder for each modality and most of them are also designed for visual language learning. This section doesn’t dive deep into the specifics of current multimodal approaches based on transformers but we provide examples for people interested in diving deep: Flamingo(visual language)(Alayrac et al. 2022), Gato(Reed et al. 2022), ImageBind(Girdhar et al. 2023), OFA(P. Wang et al. 2022), Unified-IO(Lu et al. 2022), Meta-Transformer(Y. Zhang et al. 2023), among others.\n\n\n\n\n\n\nNote\n\n\n\nVirtually all transformer challenges stem from its extreme compute and memory requirements. Truly efficient transformers such as FlashAttention could potentially alleviate those challenges."
  },
  {
    "objectID": "posts/001-transformer/index.html#footnotes",
    "href": "posts/001-transformer/index.html#footnotes",
    "title": "The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExample adapted from Deep Learning with Python by Francois Chollet↩︎\nIn the transformer paper, MLPs are what referred to as feed-forward networks(FFNs). I find the terminology of FFNs confusing sometime. MLPs are feed-forward networks but not the other way around.↩︎\nIf you want to see how embeddings look like and how words with same semantic meaning tend to be closer to each other, you can play with Embedding Projector↩︎\nThe core operation in attention is the dot product between query and keys, which, being a summation operation, is permutation invariant↩︎\nHat tip to Sebastian Raschka for sharing this in his newsletter↩︎\nBertViz be accessed at https://github.com/jessevig/bertviz↩︎\nKarpathy said that in a Twitter thread. Available here: https://twitter.com/karpathy/status/1655994367033884672?s=20↩︎\n Next sentence prediction in BERT and next token prediction in standard transformer are different. The idea is roughly similar, but the former is usually for discriminative modelling while the later is for auto-regressive generative modelling↩︎\nNext token prediction in decoder LLMs is different to next sentence prediction in BERT. The former operates on token level while the later operates on sentence level↩︎\nIt’s fair to say that GPT-3 popularized prompt engineering.↩︎\nThe inductive biases in ConvNets are the results of their translation invariance. Convolution itself is translation equivariance(changing the position of pixels changes the output) but pooling which is often used after convolution is translation invariant(changing the position of pixels doesn’t change the output) and this make the overall ConvNets translation invariant architecture↩︎\nGPU main memory is called HBM which stands for High Bandwidth Memory↩︎\nAvailable at https://github.com/tensorflow/tensor2tensor↩︎\nAvailable at https://github.com/google/trax↩︎"
  }
]