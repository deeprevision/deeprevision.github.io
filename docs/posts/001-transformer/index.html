<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jean Nyandwi">
<meta name="dcterms.date" content="2023-07-29">
<meta name="description" content="A deep dive into Transformer, a neural network architecture that was introduced in the famous paper “attention is all you need” in 2017, its applications, impacts, challenges and future directions">

<title>AI Research Blog - The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../deeprev.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XPW0YXN0CK"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XPW0YXN0CK', { 'anonymize_ip': true});
</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background: #02071a;
      }
</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="AI Research Blog - The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture">
<meta property="og:description" content="A deep dive into Transformer, a neural network architecture that was introduced in the famous paper “attention is all you need” in 2017, its applications, impacts, challenges and future directions">
<meta property="og:image" content="./transformer.png">
<meta property="og:site-name" content="AI Research Blog">
<meta name="citation_title" content="The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture">
<meta name="citation_author" content="Jean Nyandwi">
<meta name="citation_publication_date" content="2023-07-29">
<meta name="citation_cover_date" content="2023-07-29">
<meta name="citation_year" content="2023">
<meta name="citation_online_date" content="2023-07-29">
<meta name="citation_fulltext_html_url" content="https://deeprevision.github.io/posts/001-transformer/">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="Deep Learning Revision">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_journal_title=arXiv preprint arXiv:1706.03762;">
<meta name="citation_reference" content="citation_title=Text understanding from scratch;,citation_author=Xiang Zhang;,citation_author=Yann LeCun;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_journal_title=arXiv preprint arXiv:1502.01710;">
<meta name="citation_reference" content="citation_title=Layer normalization;,citation_author=Jimmy Lei Ba;,citation_author=Jamie Ryan Kiros;,citation_author=Geoffrey E Hinton;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_journal_title=arXiv preprint arXiv:1607.06450;">
<meta name="citation_reference" content="citation_title=Neural machine translation by jointly learning to align and translate;,citation_author=Dzmitry Bahdanau;,citation_author=Kyunghyun Cho;,citation_author=Yoshua Bengio;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_journal_title=arXiv preprint arXiv:1409.0473;">
<meta name="citation_reference" content="citation_title=Show, attend and tell: Neural image caption generation with visual attention;,citation_author=Kelvin Xu;,citation_author=Jimmy Ba;,citation_author=Ryan Kiros;,citation_author=Kyunghyun Cho;,citation_author=Aaron Courville;,citation_author=Ruslan Salakhutdinov;,citation_author=Richard S Zemel;,citation_author=Yoshua Bengio;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=International conference on machine learning;">
<meta name="citation_reference" content="citation_title=Neural turing machines;,citation_author=Alex Graves;,citation_author=Greg Wayne;,citation_author=Ivo Danihelka;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_journal_title=arXiv preprint arXiv:1410.5401;">
<meta name="citation_reference" content="citation_title=Effective approaches to attention-based neural machine translation;,citation_author=Minh-Thang Luong;,citation_author=Hieu Pham;,citation_author=Christopher D Manning;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_journal_title=arXiv preprint arXiv:1508.04025;">
<meta name="citation_reference" content="citation_title=Xception: Deep learning with depthwise separable convolutions;,citation_author=François Chollet;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_journal_title=arXiv preprint arXiv:1610.02357;">
<meta name="citation_reference" content="citation_title=Deep residual learning for image recognition;,citation_author=Kaiming He;,citation_author=Xiangyu Zhang;,citation_author=Shaoqing Ren;,citation_author=Jian Sun;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_conference_title=Proceedings of the IEEE conference on computer vision and pattern recognition;">
<meta name="citation_reference" content="citation_title=Batch normalization: Accelerating deep network training by reducing internal covariate shift;,citation_author=Sergey Ioffe;,citation_author=Christian Szegedy;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_conference_title=International conference on machine learning;">
<meta name="citation_reference" content="citation_title=On layer normalization in the transformer architecture;,citation_author=Ruibin Xiong;,citation_author=Yunchang Yang;,citation_author=Di He;,citation_author=Kai Zheng;,citation_author=Shuxin Zheng;,citation_author=Chen Xing;,citation_author=Huishuai Zhang;,citation_author=Yanyan Lan;,citation_author=Liwei Wang;,citation_author=Tie-Yan Liu;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference_title=International conference on machine learning;">
<meta name="citation_reference" content="citation_title=ResiDual: Transformer with dual residual connections;,citation_author=Shufang Xie;,citation_author=Huishuai Zhang;,citation_author=Junliang Guo;,citation_author=Xu Tan;,citation_author=Jiang Bian;,citation_author=Hany Hassan Awadalla;,citation_author=Arul Menezes;,citation_author=Tao Qin;,citation_author=Rui Yan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2304.14802;">
<meta name="citation_reference" content="citation_title=Dropout: A simple way to prevent neural networks from overfitting;,citation_author=Nitish Srivastava;,citation_author=Geoffrey Hinton;,citation_author=Alex Krizhevsky;,citation_author=Ilya Sutskever;,citation_author=Ruslan Salakhutdinov;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_issue=56;,citation_volume=15;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Language models are unsupervised multitask learners;,citation_author=Alec Radford;,citation_author=Jeffrey Wu;,citation_author=Rewon Child;,citation_author=David Luan;,citation_author=Dario Amodei;,citation_author=Ilya Sutskever;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=8;,citation_volume=1;,citation_journal_title=OpenAI Blog;">
<meta name="citation_reference" content="citation_title=A multiscale visualization of attention in the transformer model;,citation_author=Jesse Vig;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=Proceedings of the 57th annual meeting of the association for computational linguistics: System demonstrations;">
<meta name="citation_reference" content="citation_title=BERT: Pre-training of deep bidirectional transformers for language understanding;,citation_author=Jacob Devlin;,citation_author=Ming-Wei Chang;,citation_author=Kenton Lee;,citation_author=Kristina Toutanova;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=Proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers);">
<meta name="citation_reference" content="citation_title=Efficient transformers: A survey;,citation_author=Yi Tay;,citation_author=Mostafa Dehghani;,citation_author=Dara Bahri;,citation_author=Donald Metzler;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_journal_title=arXiv preprint arXiv:2009.06732;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2304.13712;">
<meta name="citation_reference" content="citation_title=Chain-of-thought prompting elicits reasoning in large language models;,citation_author=Jason Wei;,citation_author=Max Nye;,citation_author=Percy Liang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2201.11903;">
<meta name="citation_reference" content="citation_title=What learning algorithm is in-context learning? Investigations with linear models;,citation_author=Ekin Akyürek;,citation_author=Dale Schuurmans;,citation_author=Jacob Andreas;,citation_author=Tengyu Ma;,citation_author=Denny Zhou;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2211.15661;">
<meta name="citation_reference" content="citation_title=Larger language models do in-context learning differently;,citation_author=Jerry Wei;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Dustin Tran;,citation_author=Albert Webson;,citation_author=Yifeng Lu;,citation_author=Xinyun Chen;,citation_author=Hanxiao Liu;,citation_author=Da Huang;,citation_author=Denny Zhou;,citation_author=Tengyu Ma;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2303.03846;">
<meta name="citation_reference" content="citation_title=RoBERTa: A robustly optimized BERT pretraining approach;,citation_author=Yinhan Liu;,citation_author=Myle Ott;,citation_author=Naman Goyal;,citation_author=Jingfei Du;,citation_author=Mandar Joshi;,citation_author=Danqi Chen;,citation_author=Omer Levy;,citation_author=Mike Lewis;,citation_author=Luke Zettlemoyer;,citation_author=Veselin Stoyanov;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_journal_title=arXiv preprint arXiv:1907.11692;">
<meta name="citation_reference" content="citation_title=Language models are few-shot learners;,citation_author=Tom B Brown;,citation_author=Benjamin Mann;,citation_author=Nick Ryder;,citation_author=Melanie Subbiah;,citation_author=Jared Kaplan;,citation_author=Prafulla Dhariwal;,citation_author=Arvind Neelakantan;,citation_author=Pranav Shyam;,citation_author=Girish Sastry;,citation_author=Amanda Askell;,citation_author=others;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_journal_title=arXiv preprint arXiv:2005.14165;">
<meta name="citation_reference" content="citation_title=GPT-4 technical report;,citation_author=undefined OpenAI;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2303.08774;">
<meta name="citation_reference" content="citation_title=Improving language understanding by generative pre-training;,citation_author=Alec Radford;,citation_author=Karthik Narasimhan;,citation_author=Tim Salimans;,citation_author=Ilya Sutskever;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;">
<meta name="citation_reference" content="citation_title=Emergent abilities of large language models;,citation_author=Jason Wei;,citation_author=Yi Tay;,citation_author=Rishi Bommasani;,citation_author=Colin Raffel;,citation_author=Barret Zoph;,citation_author=Sebastian Borgeaud;,citation_author=Dani Yogatama;,citation_author=Maarten Bosma;,citation_author=Denny Zhou;,citation_author=Donald Metzler;,citation_author=Ed H. Chi;,citation_author=Tatsunori Hashimoto;,citation_author=Oriol Vinyals;,citation_author=Percy Liang;,citation_author=Jeff Dean;,citation_author=William Fedus;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2206.07682;">
<meta name="citation_reference" content="citation_title=PaLM: Scaling language modeling with pathways;,citation_author=Aakanksha Chowdhery;,citation_author=Sharan Narang;,citation_author=Jacob Devlin;,citation_author=Bosma Maarten;,citation_author=Mishra Gaurav;,citation_author=Roberts Adam;,citation_author=Barham Paul;,citation_author=Chung Hyung Won;,citation_author=Sutton Charles;,citation_author=Gehrmann Sebastian;,citation_author=Schuh Parker;,citation_author=Shi Kensen;,citation_author=Tsvyashchenko Sasha;,citation_author=Maynez Joshua;,citation_author=Rao Abhishek;,citation_author=Barnes Parker;,citation_author=Tay Yi;,citation_author=Shazeer Noam;,citation_author=Prabhakaran Vinodkumar;,citation_author=Reif Emily;,citation_author=Du Nan;,citation_author=Hutchinson Ben;,citation_author=Pope Reiner;,citation_author=Bradbury James;,citation_author=Austin Jacob;,citation_author=Isard Michael;,citation_author=Gur-Ari Guy;,citation_author=Yin Pengcheng;,citation_author=Duke Toju;,citation_author=Levskaya Anselm;,citation_author=Moreira Erica;,citation_author=Child Rewon;,citation_author=Polozov Oleksandr;,citation_author=Lee Katherine;,citation_author=Zhou Zongwei;,citation_author=Wang Xuezhi;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2204.02311;">
<meta name="citation_reference" content="citation_title=BLOOM: A 176B-parameter open-access multilingual language model;,citation_author=Teven Le Scao;,citation_author=Angela Fan;,citation_author=Christopher Akiki;,citation_author=Pavlick Ellie;,citation_author=Ilić Suzana;,citation_author=Hesslow Daniel;,citation_author=Castagné Roman;,citation_author=Luccioni Alexandra Sasha;,citation_author=Yvon François;,citation_author=Gallé Matthias;,citation_author=Tow Jonathan;,citation_author=Rush Alexander M.;,citation_author=Biderman Stella;,citation_author=Webson Albert;,citation_author=Ammanamanchi Pawan Sasanka;,citation_author=Wang Thomas;,citation_author=Sagot Benoît;,citation_author=Muennighoff Niklas;,citation_author=undefined Moral Albert Villanova;,citation_author=Ruwase Olatunji;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2211.05100;">
<meta name="citation_reference" content="citation_title=Training compute-optimal large language models;,citation_author=Jordan Hoffmann;,citation_author=Sebastian Borgeaud;,citation_author=Arthur Mensch;,citation_author=Elena Buchatskaya;,citation_author=Trevor Cai;,citation_author=Eliza Rutherford;,citation_author=Diego Las Casas;,citation_author=Lisa Anne Hendricks;,citation_author=Johannes Welbl;,citation_author=Aidan Clark;,citation_author=Tom Hennigan;,citation_author=Eric Noland;,citation_author=Katie Millican;,citation_author=George Driessche;,citation_author=Bogdan Damoc;,citation_author=Aurelia Guy;,citation_author=Simon Osindero;,citation_author=Karen Simonyan;,citation_author=Erich Elsen;,citation_author=Jack W. Rae;,citation_author=Oriol Vinyals;,citation_author=Laurent Sifre;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2203.15556;">
<meta name="citation_reference" content="citation_title=LLaMA: Open and efficient foundation language models;,citation_author=Hugo Touvron;,citation_author=Thibaut Lavril;,citation_author=Gautier Izacard;,citation_author=Xavier Martinet;,citation_author=Marie-Anne Lachaux;,citation_author=Timothée Lacroix;,citation_author=Baptiste Rozière;,citation_author=Naman Goyal;,citation_author=Eric Hambro;,citation_author=Faisal Azhar;,citation_author=Aurelien Rodriguez;,citation_author=Armand Joulin;,citation_author=Edouard Grave;,citation_author=Guillaume Lample;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2302.13971;">
<meta name="citation_reference" content="citation_title=Exploring the limits of transfer learning with a unified text-to-text transformer;,citation_author=Colin Raffel;,citation_author=Noam Shazeer;,citation_author=Adam Roberts;,citation_author=Katherine Lee;,citation_author=Sharan Narang;,citation_author=Michael Matena;,citation_author=Yanqi Zhou;,citation_author=Wei Li;,citation_author=Peter J Liu;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_journal_title=arXiv preprint arXiv:1910.10683;">
<meta name="citation_reference" content="citation_title=BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension;,citation_author=Mike Lewis;,citation_author=Yinhan Liu;,citation_author=Naman Goyal;,citation_author=Marjan Ghazvininejad;,citation_author=Abdelrahman Mohamed;,citation_author=Omer Levy;,citation_author=Veselin Stoyanov;,citation_author=Luke Zettlemoyer;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_journal_title=arXiv preprint arXiv:1910.13461;">
<meta name="citation_reference" content="citation_title=UL2: Unifying language learning paradigms;,citation_author=Yi Tay;,citation_author=Mostafa Dehghani;,citation_author=Vinh Q Tran;,citation_author=Xavier Garcia;,citation_author=Jason Wei;,citation_author=Xuezhi Wang;,citation_author=Hyung Won Chung;,citation_author=Siamak Shakeri;,citation_author=Dara Bahri;,citation_author=Tal Schuster;,citation_author=Huaixiu Steven Zheng;,citation_author=Denny Zhou;,citation_author=Neil Houlsby;,citation_author=Donald Metzler;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2205.05131;">
<meta name="citation_reference" content="citation_title=Scaling instruction-finetuned language models;,citation_author=Hyung Won Chung;,citation_author=Le Hou;,citation_author=Shayne Longpre;,citation_author=Barret Zoph;,citation_author=Yi Tay;,citation_author=William Fedus;,citation_author=Yunxuan Li;,citation_author=Xuezhi Wang;,citation_author=Mostafa Dehghani;,citation_author=Siddhartha Brahma;,citation_author=Albert Webson;,citation_author=Shixiang Shane Gu;,citation_author=Zhuyun Dai;,citation_author=Mirac Suzgun;,citation_author=Xinyun Chen;,citation_author=Aakanksha Chowdhery;,citation_author=Alex Castro-Ros;,citation_author=Marie Pellat;,citation_author=Kevin Robinson;,citation_author=Dasha Valter;,citation_author=Sharan Narang;,citation_author=Gaurav Mishra;,citation_author=Adams Yu;,citation_author=Vincent Zhao;,citation_author=Yanping Huang;,citation_author=Andrew Dai;,citation_author=Hongkun Yu;,citation_author=Slav Petrov;,citation_author=Ed H. Chi;,citation_author=Jeff Dean;,citation_author=Jacob Devlin;,citation_author=Adam Roberts;,citation_author=Denny Zhou;,citation_author=Quoc V. Le;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2210.11416;">
<meta name="citation_reference" content="citation_title=mT5: A massively multilingual pre-trained text-to-text transformer;,citation_author=Linting Xue;,citation_author=Noah Constant;,citation_author=Adam Roberts;,citation_author=Mihir Kale;,citation_author=Rami Al-Rfou;,citation_author=Aditya Siddhant;,citation_author=Aditya Barua;,citation_author=Colin Raffel;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies;">
<meta name="citation_reference" content="citation_title=SimVLM: Simple visual language model pretraining with weak supervision;,citation_author=Zirui Wang;,citation_author=Jiahui Yu;,citation_author=Adams Wei Yu;,citation_author=Zihang Dai;,citation_author=Yulia Tsvetkov;,citation_author=Yuan Cao;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2108.10904;">
<meta name="citation_reference" content="citation_title=PaLI-x: On scaling up a multilingual vision and language model;,citation_author=Xi Chen;,citation_author=Josip Djolonga;,citation_author=Piotr Padlewski;,citation_author=Basil Mustafa;,citation_author=Soravit Changpinyo;,citation_author=Jialin Wu;,citation_author=Carlos Riquelme Ruiz;,citation_author=Sebastian Goodman;,citation_author=Xiao Wang;,citation_author=Yi Tay;,citation_author=Siamak Shakeri;,citation_author=Mostafa Dehghani;,citation_author=Daniel Salz;,citation_author=Mario Lucic;,citation_author=Michael Tschannen;,citation_author=Arsha Nagrani;,citation_author=Hexiang Hu;,citation_author=Mandar Joshi;,citation_author=Bo Pang;,citation_author=Ceslee Montgomery;,citation_author=Paulina Pietrzyk;,citation_author=Marvin Ritter;,citation_author=AJ Piergiovanni;,citation_author=Matthias Minderer;,citation_author=Filip Pavetic;,citation_author=Austin Waters;,citation_author=Gang Li;,citation_author=Ibrahim Alabdulmohsin;,citation_author=Lucas Beyer;,citation_author=Julien Amelot;,citation_author=Kenton Lee;,citation_author=Andreas Peter Steiner;,citation_author=Yang Li;,citation_author=Daniel Keysers;,citation_author=Anurag Arnab;,citation_author=Yuanzhong Xu;,citation_author=Keran Rong;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2305.18565;">
<meta name="citation_reference" content="citation_title=On the opportunities and risks of foundation models;,citation_author=Rishi Bommasani;,citation_author=Drew A Hudson;,citation_author=Ehsan Adeli;,citation_author=Russ Altman;,citation_author=Simran Arora;,citation_author=Sydney Arx;,citation_author=Michael S Bernstein;,citation_author=Jeannette Bohg;,citation_author=Antoine Bosselut;,citation_author=Emma Brunskill;,citation_author=Erik Brynjolfsson;,citation_author=Shyamal Buch;,citation_author=Dallas Card;,citation_author=Rodrigo Castellon;,citation_author=Niladri Chatterji;,citation_author=Annie Chen;,citation_author=Kathleen Creel;,citation_author=Jared Quincy Davis;,citation_author=Dora Demszky;,citation_author=Chris Donahue;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2108.07258;">
<meta name="citation_reference" content="citation_title=Large language models encode clinical knowledge;,citation_author=Karan Singhal;,citation_author=Shekoofeh Azizi;,citation_author=Tao Tu;,citation_author=S Sara Mahdavi;,citation_author=Jason Wei;,citation_author=Hyung Won Chung;,citation_author=Nathan Scales;,citation_author=Ajay Tanwani;,citation_author=Heather Cole-Lewis;,citation_author=Stephen Pfohl;,citation_author=Perry Payne;,citation_author=Martin Seneviratne;,citation_author=Paul Gamble;,citation_author=Chris Kelly;,citation_author=Nathaneal Schärli;,citation_author=Aakanksha Chowdhery;,citation_author=Philip Mansfield;,citation_author=Blaise Agüera y Arcas;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2212.13138;">
<meta name="citation_reference" content="citation_title=ClinicalGPT: Large language models finetuned with diverse medical data and comprehensive evaluation;,citation_author=Guangyu Wang;,citation_author=Guoxing Yang;,citation_author=Zongxin Du;,citation_author=Longjun Fan;,citation_author=Xiaohu Li;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2306.09968;">
<meta name="citation_reference" content="citation_title=FinGPT: Open-source financial large language models;,citation_author=Hongyang Yang;,citation_author=Xiao-Yang Liu;,citation_author=Christina Dan Wang;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2306.06031;">
<meta name="citation_reference" content="citation_title=BloombergGPT: A large language model for finance;,citation_author=Shijie Wu;,citation_author=Ozan Irsoy;,citation_author=Steven Lu;,citation_author=Vadim Dabravolski;,citation_author=Mark Dredze;,citation_author=Sebastian Gehrmann;,citation_author=Prabhanjan Kambadur;,citation_author=David Rosenberg;,citation_author=Gideon Mann;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2303.17564;">
<meta name="citation_reference" content="citation_title=Galactica: A large language model for science;,citation_author=Ross Taylor;,citation_author=Marcin Kardas;,citation_author=Guillem Cucurull;,citation_author=Thomas Scialom;,citation_author=Anthony Hartshorn;,citation_author=Elvis Saravia;,citation_author=Andrew Poulton;,citation_author=Viktor Kerkez;,citation_author=Robert Stojnic;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2211.09085;">
<meta name="citation_reference" content="citation_title=Solving quantitative reasoning problems with language models;,citation_author=Aitor Lewkowycz;,citation_author=Anders Andreassen;,citation_author=David Dohan;,citation_author=Ethan Dyer;,citation_author=Henryk Michalewski;,citation_author=Vinay Ramasesh;,citation_author=Ambrose Slone;,citation_author=Cem Anil;,citation_author=Imanol Schlag;,citation_author=Theo Gutman-Solo;,citation_author=Yuhuai Wu;,citation_author=Behnam Neyshabur;,citation_author=Guy Gur-Ari;,citation_author=Vedant Misra;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2206.14858;">
<meta name="citation_reference" content="citation_title=Image transformer;,citation_author=Niki Parmar;,citation_author=Ashish Vaswani;,citation_author=Jakob Uszkoreit;,citation_author=Łukasz Kaiser;,citation_author=Noam Shazeer;,citation_author=Alexander Ku;,citation_author=Dustin Tran;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_conference_title=Proceedings of the 35th international conference on machine learning;">
<meta name="citation_reference" content="citation_title=An image is worth 16x16 words: Transformers for image recognition at scale;,citation_author=Alexey Dosovitskiy;,citation_author=Lucas Beyer;,citation_author=Alexander Kolesnikov;,citation_author=Dirk Weissenborn;,citation_author=Xiaohua Zhai;,citation_author=Thomas Unterthiner;,citation_author=Mostafa Dehghani;,citation_author=Matthias Minderer;,citation_author=Georg Heigold;,citation_author=Sylvain Gelly;,citation_author=Jakob Uszkoreit;,citation_author=Neil Houlsby;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=International conference on learning representations;">
<meta name="citation_reference" content="citation_title=Revisiting unreasonable effectiveness of data in deep learning era;,citation_author=Chen Sun;,citation_author=Abhinav Shrivastava;,citation_author=Saurabh Singh;,citation_author=Abhinav Gupta;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_conference_title=Proceedings of the IEEE international conference on computer vision;">
<meta name="citation_reference" content="citation_title=How to train your ViT? Data, augmentation, and regularization in vision transformers;,citation_author=Andreas Steiner;,citation_author=Alexander Kolesnikov;,citation_author=Xiaohua Zhai;,citation_author=Ross Wightman;,citation_author=Jakob Uszkoreit;,citation_author=Lucas Beyer;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2106.10270;">
<meta name="citation_reference" content="citation_title=Masked autoencoders are scalable vision learners;,citation_author=Kaiming He;,citation_author=Xinlei Chen;,citation_author=Saining Xie;,citation_author=Yanghao Li;,citation_author=Piotr Dollár;,citation_author=Ross Girshick;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2111.06377;">
<meta name="citation_reference" content="citation_title=Learning transferable visual models from natural language supervision;,citation_author=Alec Radford;,citation_author=Jong Wook Kim;,citation_author=Chris Hallacy;,citation_author=Aditya Ramesh;,citation_author=Gabriel Goh;,citation_author=Sandhini Agarwal;,citation_author=Girish Sastry;,citation_author=Amanda Askell;,citation_author=Pamela Mishkin;,citation_author=Jack Clark;,citation_author=Gretchen Krueger;,citation_author=Ilya Sutskever;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=International conference on machine learning;">
<meta name="citation_reference" content="citation_title=Hierarchical text-conditional image generation with CLIP latents;,citation_author=Aditya Ramesh;,citation_author=Prafulla Dhariwal;,citation_author=Alex Nichol;,citation_author=Casey Chu;,citation_author=Mark Chen;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2204.06125;">
<meta name="citation_reference" content="citation_title=High-resolution image synthesis with latent diffusion models;,citation_author=Robin Rombach;,citation_author=Andreas Blattmann;,citation_author=Dominik Lorenz;,citation_author=Patrick Esser;,citation_author=Björn Ommer;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=Proceedings of the IEEE/CVF conference on computer vision and pattern recognition;">
<meta name="citation_reference" content="citation_title=End-to-end object detection with transformers;,citation_author=Nicolas Carion;,citation_author=Francisco Massa;,citation_author=Gabriel Synnaeve;,citation_author=Nicolas Usunier;,citation_author=Alexander Kirillov;,citation_author=Sergey Zagoruyko;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_journal_title=arXiv preprint arXiv:2005.12872;">
<meta name="citation_reference" content="citation_title=Augmenting convolutional networks with attention-based aggregation;,citation_author=Hugo Touvron;,citation_author=Matthieu Cord;,citation_author=Alaaeldin El-Nouby;,citation_author=Piotr Bojanowski;,citation_author=Armand Joulin;,citation_author=Gabriel Synnaeve;,citation_author=Hervé Jégou;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2112.13692;">
<meta name="citation_reference" content="citation_title=MobileViT: Light-weight, general-purpose, and mobile-friendly vision transformer;,citation_author=Sachin Mehta;,citation_author=Mohammad Rastegari;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2110.02178;">
<meta name="citation_reference" content="citation_title=Decision transformer: Reinforcement learning via sequence modeling;,citation_author=Lili Chen;,citation_author=Kevin Lu;,citation_author=Aravind Rajeswaran;,citation_author=Kimin Lee;,citation_author=Aditya Grover;,citation_author=Michael Laskin;,citation_author=Pieter Abbeel;,citation_author=Aravind Srinivas;,citation_author=Igor Mordatch;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2106.01345;">
<meta name="citation_reference" content="citation_title=RT-1: Robotics transformer for real-world control at scale;,citation_author=Anthony Brohan;,citation_author=Noah Brown;,citation_author=Justice Carbajal;,citation_author=Yevgen Chebotar;,citation_author=Joseph Dabis;,citation_author=Chelsea Finn;,citation_author=Keerthana Gopalakrishnan;,citation_author=Karol Hausman;,citation_author=Alex Herzog;,citation_author=Jasmine Hsu;,citation_author=Julian Ibarz;,citation_author=Brian Ichter;,citation_author=Alex Irpan;,citation_author=Tomas Jackson;,citation_author=Sally Jesmonth;,citation_author=Nikhil J Joshi;,citation_author=Ryan Julian;,citation_author=Dmitry Kalashnikov;,citation_author=Yuheng Kuang;,citation_author=Isabel Leal;,citation_author=Kuang-Huei Lee;,citation_author=Sergey Levine;,citation_author=Yao Lu;,citation_author=Utsav Malla;,citation_author=Deeksha Manjunath;,citation_author=Igor Mordatch;,citation_author=Ofir Nachum;,citation_author=Carolina Parada;,citation_author=Jodilyn Peralta;,citation_author=Emily Perez;,citation_author=Karl Pertsch;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2212.06817;">
<meta name="citation_reference" content="citation_title=RoboCat: A self-improving foundation agent for robotic manipulation;,citation_author=Konstantinos Bousmalis;,citation_author=Giulia Vezzani;,citation_author=Dushyant Rao;,citation_author=Coline Devin;,citation_author=Alex X Lee;,citation_author=Maria Bauza;,citation_author=Todor Davchev;,citation_author=Yuxiang Zhou;,citation_author=Agrim Gupta;,citation_author=Akhil Raju;,citation_author=Antoine Laurens;,citation_author=Claudio Fantacci;,citation_author=Valentin Dalibard;,citation_author=Martina Zambelli;,citation_author=Murilo Martins;,citation_author=Rugile Pevceviciute;,citation_author=Michiel Blokzijl;,citation_author=Misha Denil;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2306.11706;">
<meta name="citation_reference" content="citation_title=Robust speech recognition via large-scale weak supervision;,citation_author=Alec Radford;,citation_author=Jong Wook Kim;,citation_author=Tao Xu;,citation_author=Greg Brockman;,citation_author=Christine McLeavey;,citation_author=Ilya Sutskever;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2212.04356;">
<meta name="citation_reference" content="citation_title=A generalist agent;,citation_author=Scott Reed;,citation_author=Konrad Zolna;,citation_author=Emilio Parisotto;,citation_author=Sergio Gomez Colmenarejo;,citation_author=Alexander Novikov;,citation_author=Gabriel Barth-Maron;,citation_author=Mai Gimenez;,citation_author=Yury Sulsky;,citation_author=Jackie Kay;,citation_author=Jost Tobias Springenberg;,citation_author=Tom Eccles;,citation_author=Jake Bruce;,citation_author=Ali Razavi;,citation_author=Ashley Edwards;,citation_author=Nicolas Heess;,citation_author=Yutian Chen;,citation_author=Raia Hadsell;,citation_author=Oriol Vinyals;,citation_author=Mahyar Bordbar;,citation_author=Nando Freitas;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2205.06175;">
<meta name="citation_reference" content="citation_title=ImageBind: One embedding space to bind them all;,citation_author=Rohit Girdhar;,citation_author=Alaaeldin El-Nouby;,citation_author=Zhuang Liu;,citation_author=Mannat Singh;,citation_author=Kalyan Vasudev Alwala;,citation_author=Armand Joulin;,citation_author=Ishan Misra;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2305.05665;">
<meta name="citation_reference" content="citation_title=The efficiency misnomer;,citation_author=Mostafa Dehghani;,citation_author=Anurag Arnab;,citation_author=Lucas Beyer;,citation_author=Ashish Vaswani;,citation_author=Yi Tay;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2110.12894;">
<meta name="citation_reference" content="citation_title=Transformers: State-of-the-art natural language processing;,citation_author=Thomas Wolf;,citation_author=Lysandre Debut;,citation_author=Victor Sanh;,citation_author=Julien Chaumond;,citation_author=Clement Delangue;,citation_author=Anthony Moi;,citation_author=Pierric Cistac;,citation_author=Tim Rault;,citation_author=Remi Louf;,citation_author=Morgan Funtowicz;,citation_author=Joe Davison;,citation_author=Sam Shleifer;,citation_author=Patrick Platen;,citation_author=Clara Ma;,citation_author=Yacine Jernite;,citation_author=Julien Plu;,citation_author=Canwen Xu;,citation_author=Teven Le Scao;,citation_author=Sylvain Gugger;,citation_author=Mariama Drame;,citation_author=Quentin Lhoest;,citation_author=Alexander Rush;,citation_publication_date=2020-10;,citation_cover_date=2020-10;,citation_year=2020;,citation_fulltext_html_url=https://aclanthology.org/2020.emnlp-demos.6;,citation_doi=10.18653/v1/2020.emnlp-demos.6;,citation_conference_title=Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations;,citation_conference=Association for Computational Linguistics;">
<meta name="citation_reference" content="citation_title=FlashAttention: Fast and memory-efficient exact attention with IO-awareness;,citation_author=Tri Dao;,citation_author=Daniel Y. Fu;,citation_author=Stefano Ermon;,citation_author=Atri Rudra;,citation_author=Christopher Ré;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2205.14135;">
<meta name="citation_reference" content="citation_title=FlashAttention-2: Faster attention with better parallelism and work partitioning;,citation_author=Tri Dao;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2307.08691;">
<meta name="citation_reference" content="citation_title=Extending context window of large language models via positional interpolation;,citation_author=Shouyuan Chen;,citation_author=Sherman Wong;,citation_author=Liangjian Chen;,citation_author=Yuandong Tian;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2306.15595;">
<meta name="citation_reference" content="citation_title=Do long-range language models actually use long-range context?;,citation_author=Simeng Sun;,citation_author=Kalpesh Krishna;,citation_author=Andrew Mattarella-Micke;,citation_author=Mohit Iyyer;,citation_publication_date=2021-11;,citation_cover_date=2021-11;,citation_year=2021;,citation_fulltext_html_url=https://aclanthology.org/2021.emnlp-main.62;,citation_doi=10.18653/v1/2021.emnlp-main.62;,citation_conference_title=Proceedings of the 2021 conference on empirical methods in natural language processing (EMNLP);,citation_conference=Association for Computational Linguistics;">
<meta name="citation_reference" content="citation_title=Lost in the middle: How language models use long contexts;,citation_author=Nelson F. Liu;,citation_author=Kevin Lin;,citation_author=John Hewitt;,citation_author=Ashwin Paranjape;,citation_author=Michele Bevilacqua;,citation_author=Fabio Petroni;,citation_author=Percy Liang;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2307.03172;">
<meta name="citation_reference" content="citation_title=Flamingo: A visual language model for few-shot learning;,citation_author=Jean-Baptiste Alayrac;,citation_author=Jeff Donahue;,citation_author=Pauline Luc;,citation_author=Antoine Miech;,citation_author=Iain Barr;,citation_author=Yana Hasson;,citation_author=Karel Lenc;,citation_author=Arthur Mensch;,citation_author=Katie Millican;,citation_author=Malcolm Reynolds;,citation_author=Roman Ring;,citation_author=Eliza Rutherford;,citation_author=Serkan Cabi;,citation_author=Tengda Han;,citation_author=Zhitao Gong;,citation_author=Sina Samangooei;,citation_author=Marianne Monteiro;,citation_author=Jacob Menick;,citation_author=Sebastian Borgeaud;,citation_author=Andrew Brock;,citation_author=Aida Nematzadeh;,citation_author=Sahand Sharifzadeh;,citation_author=Mikolaj Binkowski;,citation_author=Ricardo Barreira;,citation_author=Oriol Vinyals;,citation_author=Andrew Zisserman;,citation_author=Karen Simonyan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2204.14198;">
<meta name="citation_reference" content="citation_title=OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework;,citation_author=Peng Wang;,citation_author=An Yang;,citation_author=Rui Men;,citation_author=Junyang Lin;,citation_author=Shuai Bai;,citation_author=Zhikang Li;,citation_author=Jianxin Ma;,citation_author=Chang Zhou;,citation_author=Jingren Zhou;,citation_author=Hongxia Yang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=Proceedings of the 39th international conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Unified-IO: A unified model for vision, language, and multi-modal tasks;,citation_author=Jiasen Lu;,citation_author=Christopher Clark;,citation_author=Rowan Zellers;,citation_author=Roozbeh Mottaghi;,citation_author=Aniruddha Kembhavi;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2206.08916;">
<meta name="citation_reference" content="citation_title=Meta-transformer: A unified framework for multimodal learning;,citation_author=Yiyuan Zhang;,citation_author=Kaixiong Gong;,citation_author=Kaipeng Zhang;,citation_author=Hongsheng Li;,citation_author=Yu Qiao;,citation_author=Wanli Ouyang;,citation_author=Xiangyu Yue;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2307.10802;">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">AI Research Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Nyandwi" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Jeande_d" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/nyandwi" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture</h1>
                  <div>
        <div class="description">
          A deep dive into Transformer, a neural network architecture that was introduced in the famous paper “attention is all you need” in 2017, its applications, impacts, challenges and future directions
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">transformers</div>
                <div class="quarto-category">neural architectures</div>
                <div class="quarto-category">NLP</div>
                <div class="quarto-category">computer vision</div>
                <div class="quarto-category">deep learning</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="https://nyandwi.com">Jean Nyandwi</a> </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              Carnegie Mellon University
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 29, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#neural-networks-before-transformers" id="toc-neural-networks-before-transformers" class="nav-link" data-scroll-target="#neural-networks-before-transformers">Neural Networks Before Transformers</a>
  <ul class="collapse">
  <li><a href="#multilayer-perceptronsmlps" id="toc-multilayer-perceptronsmlps" class="nav-link" data-scroll-target="#multilayer-perceptronsmlps">MultiLayer Perceptrons(MLPs)</a></li>
  <li><a href="#convolutional-neural-networks" id="toc-convolutional-neural-networks" class="nav-link" data-scroll-target="#convolutional-neural-networks">Convolutional Neural networks</a></li>
  <li><a href="#recurrent-neural-networks" id="toc-recurrent-neural-networks" class="nav-link" data-scroll-target="#recurrent-neural-networks">Recurrent Neural Networks</a></li>
  </ul></li>
  <li><a href="#transformer-architecture" id="toc-transformer-architecture" class="nav-link" data-scroll-target="#transformer-architecture">Transformer Architecture</a>
  <ul class="collapse">
  <li><a href="#encoder" id="toc-encoder" class="nav-link" data-scroll-target="#encoder">Encoder</a></li>
  <li><a href="#decoder" id="toc-decoder" class="nav-link" data-scroll-target="#decoder">Decoder</a></li>
  <li><a href="#attention" id="toc-attention" class="nav-link" data-scroll-target="#attention">Attention</a>
  <ul class="collapse">
  <li><a href="#what-really-is-attention" id="toc-what-really-is-attention" class="nav-link" data-scroll-target="#what-really-is-attention">What Really is Attention?</a></li>
  <li><a href="#attention-function-query-key-value" id="toc-attention-function-query-key-value" class="nav-link" data-scroll-target="#attention-function-query-key-value">Attention Function: Query, Key, Value</a></li>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention">Multi-Head Attention</a></li>
  </ul></li>
  <li><a href="#mlps" id="toc-mlps" class="nav-link" data-scroll-target="#mlps">MLPs</a></li>
  <li><a href="#embeddings-and-positional-encoding-layers" id="toc-embeddings-and-positional-encoding-layers" class="nav-link" data-scroll-target="#embeddings-and-positional-encoding-layers">Embeddings and Positional Encoding Layers</a></li>
  <li><a href="#residual-connections-layer-normalization-and-dropout" id="toc-residual-connections-layer-normalization-and-dropout" class="nav-link" data-scroll-target="#residual-connections-layer-normalization-and-dropout">Residual Connections, Layer Normalization, and Dropout</a></li>
  <li><a href="#linear-and-softmax-layers" id="toc-linear-and-softmax-layers" class="nav-link" data-scroll-target="#linear-and-softmax-layers">Linear and Softmax Layers</a></li>
  </ul></li>
  <li><a href="#visualizing-attention" id="toc-visualizing-attention" class="nav-link" data-scroll-target="#visualizing-attention">Visualizing Attention</a></li>
  <li><a href="#the-pros-and-cons-of-attention" id="toc-the-pros-and-cons-of-attention" class="nav-link" data-scroll-target="#the-pros-and-cons-of-attention">The Pros and Cons of Attention</a></li>
  <li><a href="#large-language-transformer-models" id="toc-large-language-transformer-models" class="nav-link" data-scroll-target="#large-language-transformer-models">Large Language Transformer Models</a>
  <ul class="collapse">
  <li><a href="#evolution-of-llms" id="toc-evolution-of-llms" class="nav-link" data-scroll-target="#evolution-of-llms">Evolution of LLMs</a></li>
  <li><a href="#encoder-decoder-encoder-decoder-llms" id="toc-encoder-decoder-encoder-decoder-llms" class="nav-link" data-scroll-target="#encoder-decoder-encoder-decoder-llms">Encoder, Decoder, Encoder-decoder LLMs</a>
  <ul class="collapse">
  <li><a href="#encoder-only-llms" id="toc-encoder-only-llms" class="nav-link" data-scroll-target="#encoder-only-llms">Encoder-only LLMs</a></li>
  <li><a href="#decoder-only-llms" id="toc-decoder-only-llms" class="nav-link" data-scroll-target="#decoder-only-llms">Decoder-only LLMs</a></li>
  <li><a href="#encoder-decoder-llms" id="toc-encoder-decoder-llms" class="nav-link" data-scroll-target="#encoder-decoder-llms">Encoder-Decoder LLMs</a></li>
  </ul></li>
  <li><a href="#vertical-llms" id="toc-vertical-llms" class="nav-link" data-scroll-target="#vertical-llms">Vertical LLMs</a></li>
  </ul></li>
  <li><a href="#transformers-beyond-nlp-vision-and-other-modalities" id="toc-transformers-beyond-nlp-vision-and-other-modalities" class="nav-link" data-scroll-target="#transformers-beyond-nlp-vision-and-other-modalities">Transformers Beyond NLP: Vision and other Modalities</a></li>
  <li><a href="#transformer-current-challenges-and-future-directions" id="toc-transformer-current-challenges-and-future-directions" class="nav-link" data-scroll-target="#transformer-current-challenges-and-future-directions">Transformer: Current Challenges and Future Directions</a>
  <ul class="collapse">
  <li><a href="#efficient-transformers" id="toc-efficient-transformers" class="nav-link" data-scroll-target="#efficient-transformers">Efficient Transformers</a></li>
  <li><a href="#transformers-with-effective-long-contexts" id="toc-transformers-with-effective-long-contexts" class="nav-link" data-scroll-target="#transformers-with-effective-long-contexts">Transformers with Effective Long Contexts</a></li>
  <li><a href="#multimodal-transformer" id="toc-multimodal-transformer" class="nav-link" data-scroll-target="#multimodal-transformer">Multimodal Transformer</a></li>
  </ul></li>
  <li><a href="#open-source-implementations-of-transformer" id="toc-open-source-implementations-of-transformer" class="nav-link" data-scroll-target="#open-source-implementations-of-transformer">Open-source Implementations of Transformer</a></li>
  <li><a href="#supplementary-resources" id="toc-supplementary-resources" class="nav-link" data-scroll-target="#supplementary-resources">Supplementary Resources</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<!-- pdf title: The Transformer Blueprint: A Comprehensive Analysis, Applications, Challenges and Future Directions -->
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Invented in 2017 and first presented in the ground-breaking paper “Attention is All You Need”<span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span>, the transformer model has been a revolutionary contribution to deep learning and arguably, to computer science as a whole. Born as a tool for neural machine translation, it has proven to be far-reaching, extending its applicability beyond Natural Language Processing (NLP) and cementing its position as a versatile and general-purpose neural network architecture.</p>
<p>In this comprehensive guide, we will dissect the transformer model to its core, thoroughly exploring every key component from its attention mechanism to its encoder-decoder structure. Not stopping at the foundational level, we will traverse the landscape of large language models that leverage the power of the transformer, delving into their unique design attributes and functionalities. Further expanding the horizons, we will explore the applications of transformer models beyond NLP and probe into the current challenges and potential future directions of this influential architecture. Additionally, a curated list of open-source implementations and supplementary resources will be provided for those intrigued to explore further.</p>
<p>Without bells and whistles, let’s dive in!</p>
<p><img src="./transformer.png" class="img-fluid"> Figure 0: Transformer Architecture that we will explore in depth in this article. Adapted from <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span>.</p>
</section>
<section id="neural-networks-before-transformers" class="level1">
<h1>Neural Networks Before Transformers</h1>
<p>The designers of transformer neural architecture were interested in finding an architecture that could work for sequence to sequence modelling. It wasn’t that there weren’t existing sequence modelling architectures, it’s just that they had many drawbacks. What are other kinds of neural networks that be used for sequence modelling? What are their drawbacks? Let’s seek the answers to those questions as we motivate transformers along the way.</p>
<section id="multilayer-perceptronsmlps" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-perceptronsmlps">MultiLayer Perceptrons(MLPs)</h2>
<p>Let’s start with <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptrons(MLPs)</a>, one of the classical neural network approaches. MLPs are not super powerful themselves but you will find them integrated in almost any other architecture(surprisingly even in transformer). MLPs are basically a sequence of <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">linear layers</a> or fully connected layers.</p>
<p><img src="./mlps.png" class="img-fluid"> Figure 1: Multilayer Perceptrons(MLPs).</p>
<p>MLPs have long been used to model different kinds of data way before the AI community find best architectures for various modalities but one thing for sure, they are not suitable for sequence modelling. Due to their feedforward design, they can not preserve the order of information in a sequence. Sequence data lose meaning when the order of the data is lost. Thus, the inability of MLPs to preserve order of information make them unsuitable for sequence modelling. Also, MLPs takes lots of paramaters which is another undesired property a neural network can have.</p>
</section>
<section id="convolutional-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-neural-networks">Convolutional Neural networks</h2>
<p>Convolutional neural networks(CNNs or ConvNets) are a class of neural network architectures that are most known for processing images and other modalities such as texts and videos.</p>
<p><img src="./convnets.png" class="img-fluid"> Figure 2: Convolutional neural networks for text understanding<span class="citation" data-cites="zhang2015text">(<a href="#ref-zhang2015text" role="doc-biblioref">X. Zhang and LeCun 2015</a>)</span>.</p>
<p>ConvNets have so far been successful in small scale and large scale visual recognition but not quite successful in sequence modelling. They are easy to parallize(good for GPUs), due to their locality(computations are bundled in local parts of the input data), they require many layers to handle long-term dependencies. As opposed to images that have fixed length, most sequential data have variable length, something that neither ConvNets or MLPs can handle.</p>
</section>
<section id="recurrent-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="recurrent-neural-networks">Recurrent Neural Networks</h2>
<p>Unlike MLPs or ConvNets, recurrent neural networks(RNNs) were designed with sequence in mind. RNNs have feedback loop in their design, a key element in their ability to model sequential data. Another desirable property of RNNs is that they can handle variable length data.</p>
<p>There are fundamental problems in how RNNs are wired. Firstly, due to their sequential design, they are likely to be unstable for long-term sequences. Secondly, they can not parallized which limit their scalability on modern machine learning accelerators(like <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs</a>).</p>
<p><img src="./rnns.png" class="img-fluid"></p>
<p>Figure 3: Recurrent neural networks(RNNs).</p>
<p>Recurrent networks have many variations. One of their famous version is Long Short Term Memories(LSTMs). LSTMs can handle long-term sequences. They have a cellstate(horizontal straight line in figure below) and gates which all smooth the flow of information.</p>
<p><img src="./lstms.png" class="img-fluid"></p>
<p>Figure 4: Long Short Term Memories(LSTMs).</p>
<p>Another slightly efficient version of LSTMs is <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">gate recurrent Units(GRUs)</a>. LSTMs works great for basic sequence modelling problems but they are still limited in how far they can go. As we previously said, they can not parallized which means they can not be scaled. Also, even if they can preserve the order of information, they can not reason about the global context of the data they are processing. Context is important. Take an example in machine translation(the task that basically gave us transformer), context of sentence being translated is as important as the order.</p>
<p>All we have been doing basically is to motivate the transformers. So far, we have seen that prior neural networks were either not suitable for sequence modelling or not parallizable or not stable or limited in context length, all of which are primary desirable traits of sequence neural architectures.</p>
<p>Now that we have the right background, let’s dive into transformer architecture.</p>
</section>
</section>
<section id="transformer-architecture" class="level1">
<h1>Transformer Architecture</h1>
<p>Transformer is a neural network architecture that can process sequential data such as texts, audios, videos, and images(as a sequence of image patches). Transformer does not use any recurrent or convolution layers. It’s fundamental layer is called <strong><em>Attention</em></strong>. It also contain other basic layers such as fully-connected layers, normalization layer(LayerNorm mostly)<span class="citation" data-cites="ba2016layer">(<a href="#ref-ba2016layer" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span>, embedding layer, and positional encoding layer. We will see what each of those layers performs in next sections.</p>
<p><img src="./transformer.png" class="img-fluid"></p>
<p>Figure 5: Transformer Architecture. Adapted from <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span>.</p>
<p>As we alluded to in the beginning, transformer was initially introduced for machine translation, a task that demands processing two sequences(both input and output are sequences). Thus, the transformer model had two parts: encoder for processing the input and decoder for generating the output. More about encoder, decoder, and other layers are discussed below.</p>
<section id="encoder" class="level2">
<h2 class="anchored" data-anchor-id="encoder">Encoder</h2>
<p>Encoder is one of the main blocks of the transformer architecture that is right at the input of input sequence. Encoder transforms input sequence into compressed representation. In the orginal transformer architecture, the encoder was repeated 6 times(this depends on overall size of architecture, it can be changed). Each encoder block has 3 main layers which are multi-head attention(MHA), layer norm, and MLPs(or feedforward according to the paper).</p>
<p>Multi-head attention and MLPs are referred to as sub-layers in the transformer paper. Between sublayers, there are layer normalization and dropout and residual connections in between(refer to diagram for correct flow of those layers).</p>
<p>The number of encoder layers was 6 as said previously. The more the number of encoder layers, the larger the model, and the more the model is likely to capture the global context of the input sequences hence resulting in better task generalization.</p>
</section>
<section id="decoder" class="level2">
<h2 class="anchored" data-anchor-id="decoder">Decoder</h2>
<p>The decoder is pretty much the same as encoder except additional multi-head attention that operated over the output of the encoder. The goal of the decoder is to fuse encoder output with the target sequence and to make predictions(or to predict the next token).</p>
<p>The attention that takes the target sequence in decoder is masked to prevent the current token(being processed) from attending to subsquent tokens in the target sequence. If the decoder has access to a full target sequence, this would basically be cheating and can result in model that can not generalize beyond the training data.</p>
<p>Decoder is also typically repeated the same times as encoder. In the orginal transformer, the number of decoder blocks were also 6 blocks.</p>
</section>
<section id="attention" class="level2">
<h2 class="anchored" data-anchor-id="attention">Attention</h2>
<section id="what-really-is-attention" class="level3">
<h3 class="anchored" data-anchor-id="what-really-is-attention">What Really is Attention?</h3>
<p>Attention is the principal element of transformer architecture. In essence, attention is a mechanism that can allow the neural network to pay more attention to the part of input data that contains meaningful information and pay less attention to the rest of the input.</p>
<p>The attention mechanism was used in various tasks long before the introduction of transformer architecture. The idea of attention first appeared in neural machine translation(NMT) approach that used attention to find the set of positions in input sentence where the <em>most relevant information is concentrated</em><span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. Because their attention based NMT could align and translate jointly or simultaneously, it surprisingly performed well than previous approaches. As you can see in the image below, the network was able to find the correct order of words in a translated sentence, a feat that prior neural machine translation approaches struggled to achieve.</p>
<p><img src="./attention.png" class="img-fluid"></p>
<p>Figure 6: Aligning the source sentence and target sentence in neural machine learning translation<span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. The x-axis and y-axis shows the source sentence and translated sentence respectively. Each pixels indicates the attention weights of source(input) token with its corresponding target token. The diagonal attention represents words that are in corresponding order(ex: the agreement on the -&gt; L’ accord sur la). Attention can figure out the correct word order(ex: European Economic Area -&gt; zone économique européenne).</p>
<p>What’s going on in the image above? Can you spot something? The order of words was reversed in translated sentence wherever it make sense in target language. Thus, when translating a sentence, attention can give the model the ability to not only translate the sentence correctly, but to also translate it in the right order based on the context of the target language. In brief, attention can identify and preserve the context when translating one language to another.</p>
<p>Another earlier work that used attention is found in neural image captioning<span class="citation" data-cites="xu2015show">(<a href="#ref-xu2015show" role="doc-biblioref">Xu et al. 2015</a>)</span>. In this work, the authors used ConvNets for features extraction and RNNs with attention mechanism to generate a caption that aligns best with the input image. The image belows(taken from the paper) shows where the model roughly attends to.</p>
<p><img src="./attention-cap.png" class="img-fluid"></p>
<p>Figure 7: Generating caption with neural captioning model. The white regions show where the model is focusing when generating caption A woman is throwing a frisbee in a park”. Image from <span class="citation" data-cites="xu2015show">(<a href="#ref-xu2015show" role="doc-biblioref">Xu et al. 2015</a>)</span>.</p>
<p>On a global level, integrating attention mechanism in image captioning model helps the model to attend to the meaningful part of the input image when generating a caption.</p>
<p><img src="./attention-cap2.png" class="img-fluid"></p>
<p>Figure 8: The model can attend to key objects when generating captions. Image taken from <span class="citation" data-cites="xu2015show">(<a href="#ref-xu2015show" role="doc-biblioref">Xu et al. 2015</a>)</span>.</p>
<p>Both the examples we used above demonstrate the effectiveness of attention. Attention is really a magic mechanism that allows the neural network to focus on part of input data that contains meaningful information and focus less on rest of the input data.</p>
<p>Now that we understand attention, let’s look at the inputs of attention function in transformer architecture: querry, keys, and values.</p>
</section>
<section id="attention-function-query-key-value" class="level3">
<h3 class="anchored" data-anchor-id="attention-function-query-key-value">Attention Function: Query, Key, Value</h3>
<p>Intuitively, attention is really <em>“focus on most important part of the input data”</em>. Technically speaking, attention measures the <em>similarity</em> between two vectors and return the <em>weighted similarity scores</em>. A standard attention function takes three main inputs which are query, key, and value vectors. Before breaking down the attention function, let’s try to understand what keys, values, and queries mean.</p>
<p>Query, keys, and values are terms commonly used in search engines and database systems. To understand those terms, let’s take a simple example.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Let’s say you are searching papers that are based on <strong><em>attention</em></strong> on ArXiv. The <strong><em>query</em></strong> is ideally what you will put in the search box. Internally, the ArXiv may organize papers by a set of predefined <strong><em>keys</em></strong>. Before ArXiv gives you papers that you asked for, it will compare your <strong><em>query</em></strong> to those predefined set of keys and return papers that best match with query and keys correspondence. <strong><em>Values</em></strong> merely refers to all papers in the database. As a disclaimer, we are using this example to understand the meaning of query, keys, and values in search and database systems context. It’s not an attempt to show how ArXiv system works.</p>
<p><img src="./query-key-values.png" class="img-fluid"></p>
<p>Figure 9: Example demonstrating query, keys, and values in ArXiv paper search system.</p>
<p>With such intuitive understanding of query, keys, and values in mind, let’s move to the mathematical representation of the attention function.</p>
<p><span class="math display">\[
Attention(Q, K, V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<p>From the function above, <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, <span class="math inline">\(V\)</span> are query matrix, key matrix, value matrix respectively. We compute the dot product of query and keys and divide the product by a scaling factor of <span class="math inline">\(\sqrt{d_k}\)</span>. The scaling factor is used to avoid the scenarios where large values of <span class="math inline">\(QK^T\)</span> would result in small gradients. Then, we normalize the dot product into a probability distribution with softmax(this basically give us weighted sum) and by multiplying it with values, we get weighted values.</p>
<p><img src="./scaled-dot.png" class="img-fluid"></p>
<p>Figure 10: Graphical representation of dot-product attention. Figure adapted from <a href="https://arxiv.org/abs/1706.03762">(Vaswani, 2017)</a>.</p>
<p>The kind of attention described above is called scaled-dot product attention, a modified dot-product attention<span class="citation" data-cites="luong2015effective">(<a href="#ref-luong2015effective" role="doc-biblioref">Luong, Pham, and Manning 2015</a>)</span>. There are other kinds of attention such as additive attention<span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>, content-based attention<span class="citation" data-cites="graves2014neural">(<a href="#ref-graves2014neural" role="doc-biblioref">Graves, Wayne, and Danihelka 2014</a>)</span>, location-based attention<span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>, and general attention<span class="citation" data-cites="luong2015effective">(<a href="#ref-luong2015effective" role="doc-biblioref">Luong, Pham, and Manning 2015</a>)</span>. Each of those attention types can either be applied globally(to the whole input data), hence global attention, or locally(sub-parts of the input data), hence local attention.</p>
<p>You may have heard that transformer is parallizable and you may be wondering where it comes from. Transformer parallization comes from attention function. Provided that both query, keys, and values are matrices, attention can be performed in two main matrix multiplies and hence no loops or any recurrent operation involved. Computing attention is resonably faster for GPUs. For bigger models(in order of billions parameters) and massive training data(in order of billion/trillions tokens), attention is can be expensive since it takes quadratic <a href="https://en.wikipedia.org/wiki/Time_complexity">time complexity</a> from the fact that each token attends other tokens.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the queries, keys, and values are derived from same source, the attention applied to them is called <strong><em>self-attention</em></strong>. If they come from different source, we say <strong><em>cross-attention</em></strong>.</p>
</div>
</div>
</section>
<section id="multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention">Multi-Head Attention</h3>
<p>What we decribed above is a single attention layer. In practice, you typically would not get sound results with just one attention layer. Instead, people tend to compute multiple attention layers in parallel and concatenate the results. In nutshell, that is multi-head attention. Multi-head attention is basically multiple independent attentions computed over linearly projected QKV vectors. In the figure below of multi-head attention, the concatenated attention values are linearly projected to the model dimension.</p>
<p><img src="./mha.png" class="img-fluid"></p>
<p>Figure 11: Multi-Head attention. Figure adapted from <a href="https://arxiv.org/abs/1706.03762">(Vaswani, 2017)</a>.</p>
<p>As explained by the designers of the transformer architecture, computing multiple attentions in parallel allows the model to <em>“jointly attend to information from different representation subspaces at different positions.”“</em><span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span>. A surprising thing about multi-head attention is that it doesn’t increase the overall computation cost because the dimension of each head is oneth of number of heads(i.e, heads in base transformer is 8) of the overall model dimension(ie, 512). So, if the dimension of the model(<span class="math inline">\(d_{model}\)</span> in the paper) is 512, the number of heads in multi-head attention are 8, each head is thus <span class="math inline">\(512/8=64\)</span>.</p>
<p>Multi-head attention can be seen as depth-wise separable convolution<span class="citation" data-cites="chollet2017xception">(<a href="#ref-chollet2017xception" role="doc-biblioref">Chollet 2017</a>)</span> in ConvNets. Depth-wise separable convolution is a special type of convolution that splits input tensor into multiple channels, operate on each channel independently, concatenate the individual outputs and and feed the results to a pointwise convolution(1x1 convolution which is equivalent to a linear projection).</p>
</section>
</section>
<section id="mlps" class="level2">
<h2 class="anchored" data-anchor-id="mlps">MLPs</h2>
<p>MLPs or Multilayer Perceptrons<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> are one of the two sublayers in both encoder and decoder. MLPs in the transformer are made of two linear layers with ReLU activation in between and they are applied to each position independently and identically.</p>
<p><img src="./mlps-in-transformer.png" class="img-fluid"></p>
<p>Figure 12:Multi-Layer Perceptrons(MLP) in transformer.</p>
</section>
<section id="embeddings-and-positional-encoding-layers" class="level2">
<h2 class="anchored" data-anchor-id="embeddings-and-positional-encoding-layers">Embeddings and Positional Encoding Layers</h2>
<p>The transformer architecture incorporates two embedding layers: one at the encoder to handle the input or source sequence, and another at the decoder for handling target or output sequence. These embedding layers convert input or output tokens into dense vectors of a fixed size, essentially mapping each token in a sequence to a specific dense vector. Utilizing embeddings is a standard practice in language modeling due to the semantic depth they provide. With these embedded token vectors, those bearing similar semantic meanings tend to align in the same direction.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>The size of the embeddings in the base transformer is 512(this is the dimension of the whole model). As a side note here, transformer architecture maintains the same dimension across the whole network and it is 512 for base model. This is what referred to as <span class="math inline">\(d_{model}\)</span> previously.</p>
<p>Positional encodings serve as integral components in the initial stages of both the encoder and decoder within a Transformer model. They are used to preserve the order of tokens in a sequence. One might question the necessity of these positional embeddings. This stems from the inherent permutation invariance of the attention mechanism, whereby modifying the order of tokens does not alter the output weighted values<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. Consequently, the attention mechanism, on its own, lacks awareness of the token order. As the transformer architecture does not incorporate any other recurrence methods, positional encodings are introduced to equip the model with positional awareness of the tokens in the sequence. In essence, without positional encodings, a Transformer would indeed exhibit permutation invariance. However, such a design would fall short for tasks where sequence order holds significance, as is the case for most NLP tasks.</p>
<p>For encoding positional information in a sequence, the designers of transformer used sinusoidal functions of different frequencies. They also experimented with learned positional embeddings, but it did not make a difference in the results.</p>
</section>
<section id="residual-connections-layer-normalization-and-dropout" class="level2">
<h2 class="anchored" data-anchor-id="residual-connections-layer-normalization-and-dropout">Residual Connections, Layer Normalization, and Dropout</h2>
<p>Residual connections are at the heart of neural network design and they are one of the popular ingredients in modern deep learning. Since when deep residual networks proved substantial performance in computer vision<span class="citation" data-cites="he2016deep">(<a href="#ref-he2016deep" role="doc-biblioref">He et al. 2016</a>)</span>, residual connections have been used in almost most neural networks not just in vision but in other modalities as well. In fact, it is almost impossible to see a neural network model that does not use residual connections in present times. Residual connections alleviate unstable gradient problems and they help the model to converge faster.</p>
<p>One of the transformer authors, Ashish Vaswani once said that “residual connections carry positional information to higher layers, among other information.” Take a look at the image below!</p>
<p><img src="./resi-connect.png" class="img-fluid"></p>
<p>Figure 13: Residual connections carry signals to higher layers which improves the training of transformer model. The smooth diagonal in first image(with residuals) shows the effectiveness of residual connections. Image by Ashish Vaswani in CS224N.</p>
<p>Layer normalization<span class="citation" data-cites="ba2016layer">(<a href="#ref-ba2016layer" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span> is also one of the most used normalization techniques in modern neural networks. Layer normalization significantly reduces the training time by normalizing the activations of a layer with the layer mean and variance. Unlike batch normalization<span class="citation" data-cites="ioffe2015batch">(<a href="#ref-ioffe2015batch" role="doc-biblioref">Ioffe and Szegedy 2015</a>)</span> that normalizes each layer with mean and variance computed over the mini-batch, layer norm just normalizes each layer with the mean and variance of each activation. Layer normalization maintains similar behavior during both training and testing phases, unlike batch normalization which exhibits different behaviors in these two stages.</p>
<p>There are two ways to place layer normalization in transformer architecture. The first option is called Post layer normalization(Post-LN) where layer normalization is placed between residual blocks(or after each sublayer(multihead-attention and MLPs) but after addition). The second option is called Pre layer normalization(Pre-LN) where layer normalization is placed before each sublayer inside the residual block. The standard transformer architecture uses Post-LN, but in the <a href="https://github.com/tensorflow/tensor2tensor/commit/f5c9b17e617ea9179b7d84d36b1e8162cb369f25">updated codebase that trained the orginal transformer</a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, it was found that to be Pre-LN. This mismatch between paper and codes makes it hard to trace back the actual position of layer normalization in initial transformer but from the commit history, it looks like Pre-LN was used later. The authors could have updated the paper but they probably didn’t mind since no one knew this would turn out to be one of the influential and reference papers in neural network design.</p>
<p><img src="./pre-post-ln.png" class="img-fluid"></p>
<p>Figure 14: Post layer normalization(Post-LN) and Pre layer normalization(Pre-LN).</p>
<p>Thus, it’s not exactly clear where the layer normalization should be and this is an active research question. A recent study on the impacts of Pre-LN and Post-LN<span class="citation" data-cites="xiong2020layer">(<a href="#ref-xiong2020layer" role="doc-biblioref">Xiong et al. 2020</a>)</span> showed that placing layer normalization before multi-head attention and MLPs(Pre-LN) improves the training and converge much faster than layer normization placed after multi-head attention and MLPs. The study also claimed that with Pre-LN, you don’t need to be smart at choosing learning-rate scheduler since Pre-LN have better initializations. Neither of Pre-LN an Post-LN is perfect. Another quite recent study introduced ResDual<span class="citation" data-cites="xie2023residual">(<a href="#ref-xie2023residual" role="doc-biblioref">Xie et al. 2023</a>)</span> which basically alleviates issues of Pre-LN and Post-LN by introducing additional residual connection with layer normalization.</p>
<p>Where you should place layer normalization continue to be a question but this should be less of a question. As many people have noted, transformer seems to be a universal architecture. The orginal vanilla transformer(with few tweaks like yes LN) is the one that is still behind most novel works in language modelling, visual recognition, and multimodal learning depsite millions number of works that claims to improve the transformer. Thus, we should aim to keep the universality of this architecture. We will see this more in efficient transformers toward the end of the article.</p>
<p>Before we wrap up this section, let’s talk about dropout<span class="citation" data-cites="srivastava2014dropout">(<a href="#ref-srivastava2014dropout" role="doc-biblioref">Srivastava et al. 2014</a>)</span> in the transformer architecture. Layer normalization can acts as a regularizer as a side effect but you still need other forms of network regularizations to deal with overfitting. Dropout is applied to the output of each sublayer(before addition and normalization). It is also applied to the sum of the embeddings and the positional encodings in both encoder and decoder stacks. For other regularization techniques used in training transformer and other training details, check out the <a href="https://arxiv.org/abs/1706.03762">paper</a> for more.</p>
</section>
<section id="linear-and-softmax-layers" class="level2">
<h2 class="anchored" data-anchor-id="linear-and-softmax-layers">Linear and Softmax Layers</h2>
<p>The linear layer after decoder takes the decoded activations and project them to the size of the vocabulary. This linear layer will basically produce logits. The softmax layer will take those logits and turn them into next-token probabilities. The next predicted token will be basically the argmax of softmax output.</p>
</section>
</section>
<section id="visualizing-attention" class="level1">
<h1>Visualizing Attention</h1>
<p>Attention can capture the overall context from an input sequence, which often leads to better performance of the model. By visualizing attention, we can see which parts of the input sequence have significant influence on the model’s output. This helps us better understand how the inner workings of Transformer neural networks.</p>
<p><img src="./visualizing-attention.png" class="img-fluid"></p>
<p>Figure 15: Visualizing attention with <a href="https://huggingface.co/spaces/exbert-project/exbert">ExBert</a>.</p>
<p>The figure above depicts the attention heads on 8<span class="math inline">\(^{th}\)</span> layer of GPT-2<span class="citation" data-cites="radford2019language">(<a href="#ref-radford2019language" role="doc-biblioref">Radford et al. 2019</a>)</span>. From the figure, it’s clear that even in the early layers of the transformer, most tokens attend to each other.</p>
<p>A number of tools that visualize attention have evolved overtime to help the deep learning community understand what’s going inside the transformer model. One of the most famous tools is BertViz<span class="citation" data-cites="vig2019multiscale">(<a href="#ref-vig2019multiscale" role="doc-biblioref">Vig 2019</a>)</span> <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. <a href="https://huggingface.co/spaces/exbert-project/exbert">ExBert</a> that we used to make the above visualization is also an excellent and simple tool for visualizing the attention on most transformer based models such as GPT-2 and BERT<span class="citation" data-cites="devlin2018bert">(<a href="#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span>.</p>
</section>
<section id="the-pros-and-cons-of-attention" class="level1">
<h1>The Pros and Cons of Attention</h1>
<p>The attention mechanism has resulted in a significant shift in sequence modelling and other modalities that can be framed as sequences. When compared with other sequence networks such as recurrent networks and 1D convolutions, attention offers numerous advantages. These are briefly discussed below:</p>
<ul>
<li><p><strong>Long-term Dependencies</strong>: Traditional Recurrent Neural Networks (RNNs), including variants like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), are prone to the issue of long-term dependencies, where the model’s ability to retain information weakens over time. Attention mechanisms help mitigate this problem by enabling the model to directly access any point in the input sequence, thereby preserving the overall context.</p></li>
<li><p><strong>Parallelization</strong>: Unlike RNNs, which require sequential computation, attention-based models, such as transformer architectures, can process all tokens in the input sequence in parallel. This makes them more computationally efficient and scales better with current hardware accelerators.</p></li>
<li><p><strong>Interpretability</strong>: Attention provides a certain degree of interpretability, as it highlights the parts of the input that the model considers most important for producing a particular output. The “attention map” can help us understand what the model is “thinking.”</p></li>
<li><p><strong>Global Context</strong>: In Convolutional Neural Networks (CNNs), the receptive field is typically local and depends on the kernel size, potentially leading to the loss of broader context. However, with attention, each output token can take into account information from every token in the input sequence, thus preserving the global context.</p></li>
<li><p><strong>Improved Performance</strong>: Attention-based models, especially those that utilize transformer architectures, have achieved state-of-the-art performance in many NLP tasks, outperforming their RNN and CNN counterparts. They have also pushed envelope in other modalities such as computer vision, speech recognition, robotics, multimodal learning, etc…</p></li>
</ul>
<p>In the figure below, we summarize the properties of attention-based models versus other deep neural network architectures.</p>
<p><img src="./attention-vs-others.png" class="img-fluid"></p>
<p>Figure 16: Attention versus other recurrent network architectures. Tranformer possesses nearly all good traits of neural networks. ConvNets are close to transformer but they require many layers to achieve long-range dependencies.</p>
<p>Despite the multitude of advantages they offer, as everything else in life, attention mechanisms also come with their fair share of challenges. For instance, in several types of attention, both memory consumption and computational cost can scale quadratically with sequence length. Various strategies, such as <a href="https://openai.com/research/sparse-transformer">sparse attention</a> or local attention, have been proposed to alleviate these issues but most of them are rarely used in practice<span class="citation" data-cites="tay2020efficient">(<a href="#ref-tay2020efficient" role="doc-biblioref">Tay et al. 2020</a>)</span>.</p>
<p>While transformers offer the advantage of parallelization during training, the nature of the inference process may still necessitate a sequential approach, contingent on the specific task. Due to their autoregressive nature, transformers generate outputs one token at a time, continuing this iterative process until the desired output sequence is fully produced.</p>
<p>Furthermore, while attention offers a certain level of interpretability, it is far from perfect. Although it provides some insights into the model’s functioning, fully deciphering complex models based solely on attention maps can be, to say the least, a daunting task, if not almost impossible.</p>
</section>
<section id="large-language-transformer-models" class="level1">
<h1>Large Language Transformer Models</h1>
<section id="evolution-of-llms" class="level2">
<h2 class="anchored" data-anchor-id="evolution-of-llms">Evolution of LLMs</h2>
<p>Large Language Models (LLMs) have revolutionized human interaction with machine learning systems. Natural language interfaces, such as <a href="https://openai.com/blog/chatgpt">ChatGPT</a> and <a href="https://blog.google/technology/ai/bard-google-ai-search-updates/">Bard</a>, are powered by robust LLMs. These models have paved the way for executing natural language downstream tasks on-fly or through zero-shot learning. Such tasks, in the past, necessitated the gathering of a downstream or task-specific datasets.</p>
<p>At the core of these LLMs, it’s fundamentaly a transformer model that we have seen with little tweaks here and there. In this section, we will delve into the compressed evolution of Large Language Models. Moreover, we will explore the development of vertical LLMs, specifically designed and fine-tuned for particular applications.</p>
<p>Transformer base model had 65M parameters but since then, language models got bigger and bigger(in order of billions) and hence the name large language models. Below is a quick overview of popular large language models.</p>
<p><img src="./LLMs-table.png" class="img-fluid"></p>
<p>Figure 17: Overview of popular LLMs. Layers are number of stacked encoders/decoders or both for encoder-decoder models, width is the dimension of the model, heads are number of attention layers in multi-head attention, params are number of parameters. N.B, the numbers of heads in GPT-2 are not exactly known.</p>
<p>The training process for most large language models (LLMs) follows a broadly similar pattern. In the initial pretraining phase, LLMs are exposed to vast volumes of curated textual data, sourced from a diverse range of materials such as books, articles, code snippets, and websites. This vast dataset is essential for the models to gain a comprehensive understanding of the world, enabling them to create rich representations and generate contextually relevant responses. The general public holds high expectations for LLMs’ performance across various domains. To meet these expectations, the pretraining data must encompass a wide spectrum of topics and disciplines<span class="citation" data-cites="yang2023harnessing">(<a href="#ref-yang2023harnessing" role="doc-biblioref">J. Yang et al. 2023</a>)</span>.</p>
<p>The actual training of LLMs occurs in an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised</a> fashion, with a specific focus on <a href="https://en.wikipedia.org/wiki/Self-supervised_learning">self-supervised learning(SSL)</a>. This approach eliminates the need for labelled data, a crucial feature considering the near-impossibility of labeling the entirety of online content.</p>
<p><img src="./llms-train-workflow.png" class="img-fluid"></p>
<p>Figure 18: A typical training workflow of large language models. LLMs are typically trained on large unlabelled dataset. After, they can be used directly via prompt engineering or they can be fine-tuned further on specialized tasks.</p>
<p>However, training models on unlabelled data requires the clever implementation of training objectives since there is no ground truth for reference. Most LLMs, therefore, utilize the next-token prediction (NTP) as a common training objective. In essence, the LLMs are taught to accurately predict the next token in a sequence, gradually enhancing their understanding and generating capabilities. Another commonly used training objective is <a href="https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling">masked language modelling(MLM)</a>. Masked language models are trained to predict a masked token in a sequence. This objective was popularized by BERT<span class="citation" data-cites="devlin2018bert">(<a href="#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span>.</p>
<p>After pretraining phase, the models can be used to generate texts via techniques like zero-shot learning or few-shots learning. In zero-shot learning, a model is prompted to perform a task(or answer a given question) without any demontrations of how the task is done. In few-shots learning, a model is given a number of demonstrations of how the task is done before it can be asked to perform that task. Zero-shot learning and few-shot learning are examples of <a href="https://en.wikipedia.org/wiki/Few-shot_learning_(natural_language_processing)">in-context learning</a>. In-context learning(ICL) refers to the ability of LLMs to generate coherent texts using semantic prior knowledge<span class="citation" data-cites="wei2023larger">(<a href="#ref-wei2023larger" role="doc-biblioref">Jerry Wei et al. 2023</a>)</span> and without any parameter updates<span class="citation" data-cites="akyurek2023what">(<a href="#ref-akyurek2023what" role="doc-biblioref">Akyürek et al. 2023</a>)</span>. Prompting large language models(also known as <a href="https://en.wikipedia.org/wiki/Prompt_engineering">prompt engineering</a>) is a relatively new field itself and there are other prompt engineering techniques such as chain of thoughts(CoT)<span class="citation" data-cites="wei2022chain">(<a href="#ref-wei2022chain" role="doc-biblioref">Jason Wei, Nye, and Liang 2022</a>)</span>.</p>
<p>In-context learning tends to excel at tasks that are considered simple but falls short for tasks that can not be described easily in prompts. Complex tasks requires more than clever prompts. In the words of Karpathy, “reaching top tier performance(on complex tasks) will include finetuning, especially in applications with concrete well-defined tasks where it is possible to collect a lot of data and”practice” on it.”<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. Thus, for LLMs to get good performance on specialized tasks like mathematics, medicine, scientific fields(like chemistry), people typically finetune base LLMs on downstream datasets. We will see examples of this in the section of vertical LLMs.</p>
<p>Now that we’ve briefly introduced Large Language Models (LLMs), it’s time to examine some of the most popular LLMs, focusing specifically on their design choices: whether they function as encoders, decoders, or employ a combined encoder-decoder architecture.</p>
</section>
<section id="encoder-decoder-encoder-decoder-llms" class="level2">
<h2 class="anchored" data-anchor-id="encoder-decoder-encoder-decoder-llms">Encoder, Decoder, Encoder-decoder LLMs</h2>
<p>The standard transformer model has encoder-decoder and this has to do with the task it was meant to perform which is machine translation where you have to process both input sentence and its target translation. Since the transformer, AI research community came up with different variations of the architecture for different tasks. Depending on the task, some transformer models maintained encoder-decoder structure, some used decoder only or encoder only. Let’s start with the latter.</p>
<section id="encoder-only-llms" class="level3">
<h3 class="anchored" data-anchor-id="encoder-only-llms">Encoder-only LLMs</h3>
<p>Encoder-only LLMs use the encoder part of the standard transformer model. Encoder-only LLMs are typically used for NLP discriminative tasks such as text classification and sentiment analysis.</p>
<p>BERT<span class="citation" data-cites="devlin2018bert">(<a href="#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span> is one of most popular encoder-only language models. BERT is one of the earliest works that showed that you can pretrain a transformer(encoder) on large unlabeled text dataset and finetune the same architecture on various downstream tasks with additional task-specific head. The pretraining objectives for BERT were masked language modelling(MLM) and next sentence prediction(NSP)<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. With masked language modeling, we mask a given percentage(15% as noted in the paper) of input tokens and the goal is to predict the masked tokens. In next sentence prediction, for two sentences pair making up the input sequence, the goal is to predict whether or not two sentences are in a correct order at random.</p>
<p><img src="./mlm-bert.png" class="img-fluid"></p>
<p>Figure 19: Masked language modelling(MLM) in BERT. In the sentence example shown in the figure, the objective of training BERT is to predict the masked word “network”. In next sentence prediction objective, the workflow is roughly the same but instead of predicting the masked tokens, we predict if two sentence pairs separated by SEP token are in correct order.</p>
<p>BERT is a truly revolutionary technique that improved SOTA on ubiquitous number of NLP downstream tasks. It also inspired other efficient bidirectional architectures for NLP pretraining such as RoBERTa<span class="citation" data-cites="liu2019roberta">(<a href="#ref-liu2019roberta" role="doc-biblioref">Y. Liu et al. 2019</a>)</span> standing for Robustly optimized BERT approach. One of the main design choices that RoBERTa introduces is not using next sentence prediction objective.</p>
</section>
<section id="decoder-only-llms" class="level3">
<h3 class="anchored" data-anchor-id="decoder-only-llms">Decoder-only LLMs</h3>
<p>Decoder-only LLMs are based on the decoder part of standard transformer. In transformer architecture, decoder is highly similar to encoder except that the self-attention in decoder is masked to prevent the model to look at subsequent tokens when generating current token.</p>
<p>Decoder LLMs are trained with next token prediction objective<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. As a result, they can only generate one token at time or autoregressively. Overally, decoder models are used in generative tasks.</p>
<p>The most popular decoder models are GPT(Generative Pretrained Transformer) models family, most notably GPT-3<span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> and GPT-4<span class="citation" data-cites="openai2023gpt4">(<a href="#ref-openai2023gpt4" role="doc-biblioref">OpenAI 2023</a>)</span>. GPT-3 and GPT-4 are direct scale-up of the early GPT model<span class="citation" data-cites="radford2018improving">(<a href="#ref-radford2018improving" role="doc-biblioref">Radford et al. 2018</a>)</span>. As any other large language model, GPT models are trained on massive amount of unlabelled data(in order of billions to trillions tokens). Due to the large-scale pretraining and suitable training objective, GPT models develops impressive in-context learning capabilities where they can perform a range of NLP downstream tasks without gradient updates or task-specific fine-tuning<span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. In fact, GPT models can perform tasks like text classification, summarization, question answering <strong><em>on-fly</em></strong> by just prompting the model in zero-shot or few-shot settings<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. This remarkable feat of in-context learning has often been called “emergent abilities” of large language models<span class="citation" data-cites="wei2022emergent">(<a href="#ref-wei2022emergent" role="doc-biblioref">Jason Wei et al. 2022</a>)</span>.</p>
<p>GPT models are not the only models based on decoder. In fact, most famous LLMs are decoders. Examples include PaLM<span class="citation" data-cites="chowdhery2022palm">(<a href="#ref-chowdhery2022palm" role="doc-biblioref">Chowdhery et al. 2022</a>)</span>, BLOOM<span class="citation" data-cites="le2022bloom">(<a href="#ref-le2022bloom" role="doc-biblioref">Le Scao et al. 2022</a>)</span>, Chinchilla<span class="citation" data-cites="hoffmann2022training">(<a href="#ref-hoffmann2022training" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>, LLaMA<span class="citation" data-cites="touvron2023llama">(<a href="#ref-touvron2023llama" role="doc-biblioref">Touvron et al. 2023</a>)</span>, and many others.</p>
</section>
<section id="encoder-decoder-llms" class="level3">
<h3 class="anchored" data-anchor-id="encoder-decoder-llms">Encoder-Decoder LLMs</h3>
<p>Encoder-decoder LLMs looks like the standard transformer. They are generally used in tasks that demands processing two sequences(i.e, input and target are both sequences) such as machine translation. Encoder-decoder style is not widely used compared to other model styles we have seen. The most famous models of this kind are T5<span class="citation" data-cites="raffel2019exploring">(<a href="#ref-raffel2019exploring" role="doc-biblioref">Raffel et al. 2019</a>)</span>, BART<span class="citation" data-cites="lewis2019bart">(<a href="#ref-lewis2019bart" role="doc-biblioref">Lewis et al. 2019</a>)</span>, UL2<span class="citation" data-cites="tay2022ul2">(<a href="#ref-tay2022ul2" role="doc-biblioref">Tay et al. 2022</a>)</span>, FlanT5<span class="citation" data-cites="chung2022scaling">(<a href="#ref-chung2022scaling" role="doc-biblioref">Chung et al. 2022</a>)</span>, mT5<span class="citation" data-cites="xue2020mt5">(<a href="#ref-xue2020mt5" role="doc-biblioref">Xue et al. 2021</a>)</span>, etc…</p>
<p>Encoder-decoder style is also used in multimodal learning, most notably vision-language pretraining(VLP). Works like SimVLM<span class="citation" data-cites="wang2021simvlm">(<a href="#ref-wang2021simvlm" role="doc-biblioref">Z. Wang et al. 2021</a>)</span> and PaLI-X<span class="citation" data-cites="chen2023palix">(<a href="#ref-chen2023palix" role="doc-biblioref">X. Chen et al. 2023</a>)</span> employs encoder for learning joint image and text representations and decoder for generating the output.</p>
</section>
</section>
<section id="vertical-llms" class="level2">
<h2 class="anchored" data-anchor-id="vertical-llms">Vertical LLMs</h2>
<p>Most of LLMs that we outlined above are typically referred to as foundational or <a href="https://openai.com/blog/frontier-model-forum">frontier</a> LLMs. Foundational models are typically trained on massive amount of data with self-supervision and they can be fine-tuned to a wide range of downstream tasks<span class="citation" data-cites="bommasani2022opportunities">(<a href="#ref-bommasani2022opportunities" role="doc-biblioref">Bommasani et al. 2022</a>)</span>.</p>
<p>Vertical LLMs are a class of LLMs that are adapted to specific applications. Foundational LLMs can generalize to simple tasks like sentiment analysis but they don’t perform well on complex tasks or tasks that require a domain expertize. For example, a foundational LLM is unlikely to perform well on medical question answering task because it doesn’t have expertize in medicine. More examples: a foundational LLM is unlikely to perform well on legal question answering task because it doesn’t have expertize in law. This is also true in other fields such as finance, physics, chemistry, etc…Vertical LLMs are designed to address this issue. They are trained on a specific domain and they can perform well on tasks that require expertize in that domain. Foundational models aim to be generalists but most of the time, we care about models that can do one thing very well.</p>
<p>Examples of recent vertical LLMs include MedPaLM<span class="citation" data-cites="singhal2022large">(<a href="#ref-singhal2022large" role="doc-biblioref">Singhal et al. 2022</a>)</span> and <a href="https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/">Med-PaLM 2</a>, ClinicalGPT<span class="citation" data-cites="wang2023clinicalgpt">(<a href="#ref-wang2023clinicalgpt" role="doc-biblioref">G. Wang et al. 2023</a>)</span>, FinGPT<span class="citation" data-cites="yang2023fingpt">(<a href="#ref-yang2023fingpt" role="doc-biblioref">H. Yang, Liu, and Wang 2023</a>)</span>, BloombergGPT<span class="citation" data-cites="wu2023bloomberggpt">(<a href="#ref-wu2023bloomberggpt" role="doc-biblioref">Wu et al. 2023</a>)</span>, Galactica<span class="citation" data-cites="taylor2022galactica">(<a href="#ref-taylor2022galactica" role="doc-biblioref">Taylor et al. 2022</a>)</span>, Minerva<span class="citation" data-cites="lewkowycz2022solving">(<a href="#ref-lewkowycz2022solving" role="doc-biblioref">Lewkowycz et al. 2022</a>)</span>, among others.</p>
<p><img src="./llms-topologies.png" class="img-fluid"></p>
<p>Figure 20: LLMs Topologies. Adapted from <span class="citation" data-cites="yang2023harnessing">(<a href="#ref-yang2023harnessing" role="doc-biblioref">J. Yang et al. 2023</a>)</span>.</p>
</section>
</section>
<section id="transformers-beyond-nlp-vision-and-other-modalities" class="level1">
<h1>Transformers Beyond NLP: Vision and other Modalities</h1>
<p>Transformer was introduced for Natural Language Processing(NLP) domain, more precisely, for neural machine translation. In no time, transformers outperformed prior neural networks on most NLP tasks and quickly expanded into other modalities. In this section, we will discuss in brief the emergence of transformers in visual recognition and other modalities.</p>
<p>Visual recognition is one of the earliest modalities that was significantly impacted by transformers. For a long time, ConvNets were state of the arts in visual recognition. It’s thus a critical to ask why researchers care about alternatives to ConvNets. The main downside of ConvNets is their spatial <a href="https://en.wikipedia.org/wiki/Inductive_bias">inductive biases</a><a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>.</p>
<p>One of the earliest applications of transformer to image processing is Image Transformer <span class="citation" data-cites="parmar2018image">(<a href="#ref-parmar2018image" role="doc-biblioref">Parmar et al. 2018</a>)</span> which approached image generation as an autoregressive problem, analogous to text generation. The Image Transformer was a standard transformer applied to a sequence of pixels, trained to generate these pixels autoregressively until it created the complete image. This was a great idea, but as it turns out, images typically have large resolutions, and thus, it was not feasible to apply self-attention to images of 256x256 for instance. There were several works attempting to apply transformer to image domain but one of the first successful works was Vision Transformer<span class="citation" data-cites="dosovitskiy2020image">(<a href="#ref-dosovitskiy2020image" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span> that applied the transformer encoder to a sequence of images patches. ViT overcame the computational complexities of self-attention by image patchification idea, marking a significant step in extending transfomers to computer vision domain.</p>
<p>As we saw early, a huge contribution of transformers successes in NLP was unsupervised pretraining on massive amount of unlabelled data. The success of Vision Transfomer was also attributed to millions of training images, JFT-300M<span class="citation" data-cites="sun2017revisiting">(<a href="#ref-sun2017revisiting" role="doc-biblioref">C. Sun et al. 2017</a>)</span> although later works like MAE<span class="citation" data-cites="he2021masked">(<a href="#ref-he2021masked" role="doc-biblioref">He et al. 2021</a>)</span> and <span class="citation" data-cites="steiner2021train">(<a href="#ref-steiner2021train" role="doc-biblioref">Steiner et al. 2021</a>)</span> achieved resonably good performance on classical computer vision benchmarks such as ImageNet. MAE is an encoder-decoder self-supervised model that follows BERT pretraining objective of predicting randomly masked patches while the later explores clever augmentations and regularizations to train ViT. ViT has been used as backbone in many influential papers such as CLIP<span class="citation" data-cites="radford2021learning">(<a href="#ref-radford2021learning" role="doc-biblioref">Radford et al. 2021</a>)</span>, DALLE•2<span class="citation" data-cites="ramesh2022hierarchical">(<a href="#ref-ramesh2022hierarchical" role="doc-biblioref">Ramesh et al. 2022</a>)</span>, Stable Diffusion<span class="citation" data-cites="rombach2022high">(<a href="#ref-rombach2022high" role="doc-biblioref">Rombach et al. 2022</a>)</span>, among other recent works in visual language models. Aside from ViT enabling joint modelling of vision and language, it has also been augmented with convolutional neural networks to get both worlds in computer vision downstream tasks. Notable works of ConvNets and Vision Transformer topology are DETR<span class="citation" data-cites="carion2020end">(<a href="#ref-carion2020end" role="doc-biblioref">Carion et al. 2020</a>)</span>, PatchConvNet<span class="citation" data-cites="touvron2021augmenting">(<a href="#ref-touvron2021augmenting" role="doc-biblioref">Touvron et al. 2021</a>)</span>, MobileViT<span class="citation" data-cites="mehta2022mobilevit">(<a href="#ref-mehta2022mobilevit" role="doc-biblioref">Mehta and Rastegari 2022</a>)</span>, among others.</p>
<p>Vision and language are two of the most important modalities when it comes to human to computer interaction and it’s not surprising that most works incorporating transformers have been in language, vision, or visual language learning. That said, transformers have been used in other modalities such as reinforcement learning<span class="citation" data-cites="chen2021decision">(<a href="#ref-chen2021decision" role="doc-biblioref">L. Chen et al. 2021</a>)</span>, robotics(<span class="citation" data-cites="brohan2022rt">(<a href="#ref-brohan2022rt" role="doc-biblioref">Brohan et al. 2022</a>)</span>, RoboCat<span class="citation" data-cites="bousmalis2023robocat">(<a href="#ref-bousmalis2023robocat" role="doc-biblioref">Bousmalis et al. 2023</a>)</span>), and speech recognition<span class="citation" data-cites="radford2022robust">(<a href="#ref-radford2022robust" role="doc-biblioref">Radford et al. 2022</a>)</span>. Finally, works such as Gato<span class="citation" data-cites="reed2022generalist">(<a href="#ref-reed2022generalist" role="doc-biblioref">Reed et al. 2022</a>)</span> and ImageBind<span class="citation" data-cites="girdhar2023imagebind">(<a href="#ref-girdhar2023imagebind" role="doc-biblioref">Girdhar et al. 2023</a>)</span> have gone further in modelling pretty much all modalities.</p>
<p>Transformer has established itself as universal architecture and recent works across different modalities prove that, but there are still challenges.</p>
</section>
<section id="transformer-current-challenges-and-future-directions" class="level1">
<h1>Transformer: Current Challenges and Future Directions</h1>
<section id="efficient-transformers" class="level2">
<h2 class="anchored" data-anchor-id="efficient-transformers">Efficient Transformers</h2>
<p>Transformer has shown significant performance across various modalities such as language, vision, robotics, and reinforcement learning. Transformer neural network architecture has a set of traits that make it a suitable architecture for those domains: it is expressive, plays well with current optimization techniques, and it can be parallized. From those traits, one can say that transformer is an efficient architecture. That said however, the efficiency of transformer comes with enormous computatation cost due to the quadratic time and memory complexity of self-attention. The compute requirements of transformer has limited its scalability and its applications in low-budget devices such as smartphones and microcontrollers.</p>
<p>Model efficiency is an important thing to take into account when developing and deploying machine learning systems because how a model perform during inference can affects user experience<span class="citation" data-cites="dehghani2022efficiency">(<a href="#ref-dehghani2022efficiency" role="doc-biblioref">Dehghani et al. 2022</a>)</span>. There has been zillion transformer models that claim to improve the efficiency(memory footprint and computational cost) of transformer architecture(those models are typically called <strong><em>“xformers”</em></strong>) but those models usually tend to be targeted at one particular benchmark or device. Most of the new <strong><em>xformers</em></strong> models that claim to reduce the quadratic time and memory complexity of self-attention are much slower than vanilla transformer and they are rarely used in practice and they don’t have the universality of original transformer<span class="citation" data-cites="tay2020efficient">(<a href="#ref-tay2020efficient" role="doc-biblioref">Tay et al. 2020</a>)</span>.</p>
<p>As <span class="citation" data-cites="tay2020efficient">(<a href="#ref-tay2020efficient" role="doc-biblioref">Tay et al. 2020</a>)</span> puts it nicely in a survey of “Efficient Transformers”, <em>the ideal xformer should yes reduce the quadratic time complexity of self-attention, but should stay universal and perform well across all tasks and modalities. It should also not trade-off speed for memory, should not be hard-engineered, should stay elegant and simple.</em> For more, I recommend you read the survey paper of <a href="https://arxiv.org/abs/2009.06732">efficient transformers</a>.</p>
<p><img src="./efficient-trans.png" class="img-fluid"> Figure 21: A taxonomy of efficient transformers. Image from <span class="citation" data-cites="tay2020efficient">(<a href="#ref-tay2020efficient" role="doc-biblioref">Tay et al. 2020</a>)</span> .</p>
<p>Virtually all modified transformer models compute the approximation of attention to reduce the cost down. As opposed to those approaches, there is actually one kind of attention that computes exact standard attention values but way faster. That approach is FlashAttention<span class="citation" data-cites="dao2022flashattention">(<a href="#ref-dao2022flashattention" role="doc-biblioref">Dao et al. 2022</a>)</span> and we will talk about it on a high-level.</p>
<p>FlashAttention is fast and memory-efficient algorithm that computes the exact attention. FlashAttention is 2-4x faster than standard attention. It achieves this enormous increase in compute efficiency by using two main techniques: tiling and recomputation. Tiling happens in forward pass and it involves splitting large matrices in attention(K key and V value) into blocks. Rather than computing attention over entire matrices, FlashAttention computes it over blocks and concatenate the resulting blocks saving a huge amount of memory. Recomputation happens in backward pass and it basically means recomputing the attention matrix rather than storing it in forward. The idea of FlashAttention boils down to improving the memory and not decreasing computations because modern GPUs have high theorical FLOPs(Floaping Point Operations, means you want to max that out) but limited memory<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>(means any saving in memory can improve the training speed). HBM(High Bandwidth Memory) is typically large but it is not faster than on-chip SRAM(Static Random Access Memory) and thus, the computations over blocks(of K and V) happens in SRAM(because it is faster) but all full matrices are stored in HBM(because it’s big). This high-level explanation is probably oversimplication provided that FlashAttention is implemented at the GPU level(with CUDA software) and this is in fact the reason why it is IO aware but hopefully that explains what’s going on in this fast algorithm.</p>
<p>Below image shows the memory hierarchy in GPU, FlashAttention algorithm, and amount of time(in ms) taken by each intermediate step in GPT-2 attention versus FlashAttention. Ideally, we would want the bulk of computations to be taken by matrix multiplication(matmul) operations but surprisingly, dropout, softmax, and mask(i.e, GPT-2 is decoder model) end up taking the whole runtime in GPT-2 attention because they are computed over full matrices. Matmuls take less runtime than those other operations because GPUs are exactly designed to be fast at matrix multiplications(they have really high theorical FLOPs and maximizing FLOPs usage doesn’t reduce the runtime). By using tiling and recomputation techniques, the compute time of FlashAttention is significantly low compared to standard attention as you can see below.</p>
<p><img src="./flash-attn.png" class="img-fluid"></p>
<p>Figure 22: The memory hierachy in GPU, FlashAttention algorithm, and runtime of GPT-2 attention vs FlashAttention.</p>
<p>FlashAttention is intergrated in PyTorch 2.0, Hugging Face transformers, Microsoft’s DeepSpeed, MosaicML composer library and <a href="https://github.com/Dao-AILab/flash-attention/blob/main/usage.md">many other library</a>. You can learn more FlashAttention in the paper, or watch <a href="https://www.youtube.com/watch?v=gMOAud7hZg4">this video</a> by core author, and the <a href="https://www.adept.ai/blog/flashier-attention">release blogpost</a>. At the time of writing this section, FlashAttention2<span class="citation" data-cites="dao2023flashattention">(<a href="#ref-dao2023flashattention" role="doc-biblioref">Dao 2023</a>)</span> was also released and it is even faster than FlashAttention version 1 on several orders of magnitude. FlashAttention-2 improves parallelism by parallelizing over sequence length dimension instead of batch size and number of attention heads and splits Q(query) matrix instead of K and V. This <a href="https://crfm.stanford.edu/2023/07/17/flash2.html">release blog post</a> explains well what FlashAttention2 brings to the tensor table.</p>
</section>
<section id="transformers-with-effective-long-contexts" class="level2">
<h2 class="anchored" data-anchor-id="transformers-with-effective-long-contexts">Transformers with Effective Long Contexts</h2>
<p>Handling long context length is one of the main active areas of research in Transformer large models. As direct consequence of the quadratic time and memory complexity of attention, transformer fails to process long context windows. Researching techniques that extend the context window of transformer architecture is an important thing since context window determines the amount of information that you can fit in transformer memory during inference. Tasks like long conversations, summarizing long documents, and executing long-term planning may require models that support long context windows<span class="citation" data-cites="chen2023extending">(<a href="#ref-chen2023extending" role="doc-biblioref">S. Chen et al. 2023</a>)</span>.</p>
<p>Alot have been written about context windows and extending them such as <span class="citation" data-cites="sun-etal-2021-long">(<a href="#ref-sun-etal-2021-long" role="doc-biblioref">S. Sun et al. 2021</a>)</span>, but I want to highlight a recent paper that presents remarkable findings around long contexts. Recent language models(based on transformer) can take longer contexts but it’s not clear whether long context actually helps. As shown by <span class="citation" data-cites="liu2023lost">(<a href="#ref-liu2023lost" role="doc-biblioref">N. F. Liu et al. 2023</a>)</span>, the performance of language models degrades with increase in input context length. So, even for models that have extended context length, their performance still degrades for longer input contexts. Also, the work also found that language models perform well when the relevant information are placed at the beginning or the end of the input context and significantly degrades when the relevant information are placed in the middle, suggesting that language models are U-shaped reasoners.</p>
<p>The findings highlighted above are appealing and provide broad implications that could be applicable in the design of fine-tuning datasets and during in-context learning, but it’s important to note that none of those is established understandings provided that “how transformer models perform on long context windows” is an active area of research. We hope that future transformer models will be able to operate over long input sequences and at the same time perform well regardless of relevant information are placed. This is in fact the holy grail of large language models.</p>
<p><img src="./context.png" class="img-fluid"> Figure 23: Language models(based on transformer) tends to perform well when relevant information are at the beginning or at the end of input context(graph on the left) and their performance decreases for longer contexts(graph on the right). The graphs are taken from <span class="citation" data-cites="liu2023lost">(<a href="#ref-liu2023lost" role="doc-biblioref">N. F. Liu et al. 2023</a>)</span>.</p>
</section>
<section id="multimodal-transformer" class="level2">
<h2 class="anchored" data-anchor-id="multimodal-transformer">Multimodal Transformer</h2>
<p>A primary objective in neural network design is to architect a single, universal model that can efficiently process multiple modalities without necessitating modality-specific encoders or preprocessing. Indeed, transformer models have seen widespread application across various domains, spanning text, images, robotics, and speech. Yet, the goal of creating a truly universal transformer — one that performs equally effectively across all modalities without requiring specific adjustments — remains a challenge. This challenge arises from the inherent differences and complexities in data types and the transformer model itself, which frequently demand modality-specific modifications.</p>
<p>For instance, the process for handling text, images, and speech each have unique considerations due to their individual characteristics. Transformers excel in scenarios where data can be framed as a sequence of tokens, however, the method of transposing a particular modality into such a sequence significantly varies among different modalities. Consequently, the challenge lies in designing a singular architecture that can uniformly extract valuable insights from all data types with comparable efficiency.</p>
<p>The achievement of such an architecture would signify a monumental stride in the field of multimodal learning, paving the way for models that can seamlessly transition between different types of data and potentially unlocking new avenues of exploration in multimodal representation learning.</p>
<p>Nearly all current state-of-the-arts in multimodal learning typically uses separate tokenizer and encoder for each modality and most of them are also designed for visual language learning. This section doesn’t dive deep into the specifics of current multimodal approaches based on transformers but we provide examples for people interested in diving deep: Flamingo(visual language)<span class="citation" data-cites="alayrac2022flamingo">(<a href="#ref-alayrac2022flamingo" role="doc-biblioref">Alayrac et al. 2022</a>)</span>, Gato<span class="citation" data-cites="reed2022generalist">(<a href="#ref-reed2022generalist" role="doc-biblioref">Reed et al. 2022</a>)</span>, ImageBind<span class="citation" data-cites="girdhar2023imagebind">(<a href="#ref-girdhar2023imagebind" role="doc-biblioref">Girdhar et al. 2023</a>)</span>, OFA<span class="citation" data-cites="wang2022ofa">(<a href="#ref-wang2022ofa" role="doc-biblioref">P. Wang et al. 2022</a>)</span>, Unified-IO<span class="citation" data-cites="lu2022unified">(<a href="#ref-lu2022unified" role="doc-biblioref">Lu et al. 2022</a>)</span>, Meta-Transformer<span class="citation" data-cites="zhang2023meta">(<a href="#ref-zhang2023meta" role="doc-biblioref">Y. Zhang et al. 2023</a>)</span>, among others.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Virtually all transformer challenges stem from its extreme compute and memory requirements. Truly efficient transformers such as FlashAttention could potentially alleviate those challenges.</p>
</div>
</div>
</section>
</section>
<section id="open-source-implementations-of-transformer" class="level1">
<h1>Open-source Implementations of Transformer</h1>
<p>The original transformer model was implemented in Tensor2Tensor library<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> but this was deprecated recently. The successor of of Tensor2Tensor is Trax which is based on JAX<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
<p>There are many open-source implementations of transformer model architecture. Let’s briefly talk about three of most popular implementations. HuggingFace <a href="https://github.com/huggingface/transformers">Transformer</a> library<span class="citation" data-cites="wolf-etal-2020-transformers">(<a href="#ref-wolf-etal-2020-transformers" role="doc-biblioref">Wolf et al. 2020</a>)</span> is arguably one of the most popular implementations of transformers. The library simplifies inference pipelines for NLP(and vision) downstream tasks and can be used to train or finetune transformer-based models. HuggingFace Transformer library is easy to use, it’s clean, and has a large community of developers and contributors. <a href="https://github.com/karpathy/minGPT">minGPT</a> and <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a> by Andrej Karpathy are also popular implementations in open-source and research community. Furthermore, <a href="https://github.com/lucidrains/x-transformers">x-transformers</a> provides concise and experimental implementations of various transformer models usually from new research papers.</p>
<p>Lastly, it’s unlikely you will need to implement transformer model or part of it from scratch because modern deep learning frameworks such as <a href="https://pytorch.org/docs/stable/nn.html#transformer-layers">PyTorch</a>, <a href="keras.io/api/keras_nlp">Keras</a>, and JAX(Via <a href="https://github.com/google/flax">Flax</a>) provides its implementation as layers that you can import easily just like how you import convolution or linear layers.</p>
</section>
<section id="supplementary-resources" class="level1">
<h1>Supplementary Resources</h1>
<p>This article contributes to an existing pool of knowledge surrounding the understanding of transformer neural network architecture. Therefore, it would be remiss not to highlight some invaluable resources on transformer architecture, which we will briefly provides below:</p>
<ul>
<li><p>The Annotated Transformer: This is one of the best and practical resources. It provides line-by-line implementation of transformer architecture with completely usable code. The <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">original version</a> was written by Sasha Rush and <a href="http://nlp.seas.harvard.edu/annotated-transformer/">recent version</a> was written by Austin Huang and his colleagues.</p></li>
<li><p>Let’s Build GPT from Scratch by Andrej Karpathy: This is arguably the best resource regarding implementations of transformer, most notably, GPT(Generative Pre-training Transformer). Karpathy builds and trains entire GPT from scratch, providing a decent explanation of every step along the way. Here is a <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&amp;index=7&amp;t=1939s">lecture video</a> and <a href="https://github.com/karpathy/nanoGPT/tree/master">accompanying code repository(nanoGPT)</a>.</p></li>
<li><p><a href="https://web.stanford.edu/class/cs25/">Stanford CS25: Transformers United V2</a> aims at examining how transformers work and how they are applied in different fields from NLP, CV, biology to robotics and more. This course contains excellent talks from researchers. The <a href="https://www.youtube.com/watch?v=XfpMkf4rD6E">introductory class</a> of recent version of the course delves into transformer architecture and it is given by Karpathy, someone who deeply understands the intricacies of neural networks.</p></li>
<li><p><a href="https://arxiv.org/abs/2207.09238">Formal Algorithms for Transformers</a> provides a mathematical overview and formal algorithms of various transformer architectures.</p></li>
<li><p><a href="https://kipp.ly/transformer-taxonomy/">Transformer Taxonomy</a> provides an excellent literature review of transformer models, architectural changes since the inception of standard transformer, post pre-training techniques and 3 training techniques.</p></li>
<li><p>The Illustrated Transformer is a remarkable <a href="https://jalammar.github.io/illustrated-transformer/">blog post</a> that break the transformer model apart and explains each part intuitively.</p></li>
<li><p>Transformer and attention blog series by Lilian Weng also provide excellent understanding of transformer and attention mechanism. A notable example of relevant Lilian Weng blogs are <a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">The Transformer Family Version</a>(there is also <a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">version 2</a> of this blog) and <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention? Attention!</a>.</p></li>
<li><p>Attention is All You Need <a href="https://www.youtube.com/watch?v=iDulhoQ2pro">Video</a> by Yannic Kilcher walkthroughs the paper, explaining all the relevant concepts and related works well.</p></li>
<li><p><a href="https://arxiv.org/abs/2302.07730">Transformer models: an introduction and catalog</a> is also another resource that is worth mentioning. It provides a decent catalog of popular transformer models.</p></li>
</ul>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>The significance of transformer neural network architecture can not be overstated in the field of deep learning and computer science. The transformer model, initially introduced for neural machine translation has evolved into a versatile and general-purpose architecture, demonstrating impressive performance beyond natural language processing into other various modalities.</p>
<p>Throughout this article, we have delved into the core mechanics of the transformer and its essential components - its encoder and decoder structure, attention mechanism, multi-head attention, MLPs, embedding, positional encoding layers, and more. We have explored several benefits of self-attention, along with potential drawbacks. Also, by examining the visualization of attention, we have gained a deeper understanding of how transformers focus on different parts of the input sequence to generate outputs.</p>
<p>Transformers are at the core of large language models(LLMs) which has taken the world by a storm recently. We have seen evolution of LLMs and their different design styles, and the applications of transformers beyond NLP. We have also talked their current challenges, including the need for more efficient models and the effective use of context window. These challenges present exciting opportunities for future research and improvements.</p>
<p>As deep learning field continues to evolve, transformer architecture remains a foundational building block of modern machine learning systems. There are many variations of transformer architectures, but regardless of what the future of transformers holds, one thing has been certain - attention is all you need. Stay curious, keep learning, and always pay attention!</p>
</section>
<section id="references" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-akyurek2023what" class="csl-entry" role="listitem">
Akyürek, Ekin, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023. <span>“What Learning Algorithm Is in-Context Learning? Investigations with Linear Models.”</span> <em>arXiv Preprint arXiv:2211.15661</em>.
</div>
<div id="ref-alayrac2022flamingo" class="csl-entry" role="listitem">
Alayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. <span>“Flamingo: A Visual Language Model for Few-Shot Learning.”</span> <em>arXiv Preprint arXiv:2204.14198</em>.
</div>
<div id="ref-ba2016layer" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. <span>“Layer Normalization.”</span> <em>arXiv Preprint arXiv:1607.06450</em>.
</div>
<div id="ref-bahdanau2014neural" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <em>arXiv Preprint arXiv:1409.0473</em>.
</div>
<div id="ref-bommasani2022opportunities" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, et al. 2022. <span>“On the Opportunities and Risks of Foundation Models.”</span> <em>arXiv Preprint arXiv:2108.07258</em>.
</div>
<div id="ref-bousmalis2023robocat" class="csl-entry" role="listitem">
Bousmalis, Konstantinos, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X Lee, Maria Bauza, Todor Davchev, et al. 2023. <span>“RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation.”</span> <em>arXiv Preprint arXiv:2306.11706</em>.
</div>
<div id="ref-brohan2022rt" class="csl-entry" role="listitem">
Brohan, Anthony, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, et al. 2022. <span>“RT-1: Robotics Transformer for Real-World Control at Scale.”</span> <em>arXiv Preprint arXiv:2212.06817</em>.
</div>
<div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>arXiv Preprint arXiv:2005.14165</em>.
</div>
<div id="ref-carion2020end" class="csl-entry" role="listitem">
Carion, Nicolas, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. <span>“End-to-End Object Detection with Transformers.”</span> <em>arXiv Preprint arXiv:2005.12872</em>.
</div>
<div id="ref-chen2021decision" class="csl-entry" role="listitem">
Chen, Lili, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. <span>“Decision Transformer: Reinforcement Learning via Sequence Modeling.”</span> <em>arXiv Preprint arXiv:2106.01345</em>.
</div>
<div id="ref-chen2023extending" class="csl-entry" role="listitem">
Chen, Shouyuan, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. <span>“Extending Context Window of Large Language Models via Positional Interpolation.”</span> <em>arXiv Preprint arXiv:2306.15595</em>.
</div>
<div id="ref-chen2023palix" class="csl-entry" role="listitem">
Chen, Xi, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, et al. 2023. <span>“PaLI-x: On Scaling up a Multilingual Vision and Language Model.”</span> <em>arXiv Preprint arXiv:2305.18565</em>.
</div>
<div id="ref-chollet2017xception" class="csl-entry" role="listitem">
Chollet, François. 2017. <span>“Xception: Deep Learning with Depthwise Separable Convolutions.”</span> <em>arXiv Preprint arXiv:1610.02357</em>.
</div>
<div id="ref-chowdhery2022palm" class="csl-entry" role="listitem">
Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Bosma Maarten, Mishra Gaurav, Roberts Adam, Barham Paul, et al. 2022. <span>“PaLM: Scaling Language Modeling with Pathways.”</span> <em>arXiv Preprint arXiv:2204.02311</em>.
</div>
<div id="ref-chung2022scaling" class="csl-entry" role="listitem">
Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, et al. 2022. <span>“Scaling Instruction-Finetuned Language Models.”</span> <em>arXiv Preprint arXiv:2210.11416</em>.
</div>
<div id="ref-dao2023flashattention" class="csl-entry" role="listitem">
Dao, Tri. 2023. <span>“FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.”</span> <em>arXiv Preprint arXiv:2307.08691</em>.
</div>
<div id="ref-dao2022flashattention" class="csl-entry" role="listitem">
Dao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. <span>“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.”</span> <em>arXiv Preprint arXiv:2205.14135</em>.
</div>
<div id="ref-dehghani2022efficiency" class="csl-entry" role="listitem">
Dehghani, Mostafa, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi Tay. 2022. <span>“The Efficiency Misnomer.”</span> <em>arXiv Preprint arXiv:2110.12894</em>.
</div>
<div id="ref-devlin2018bert" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, 4171–86.
</div>
<div id="ref-dosovitskiy2020image" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> In <em>International Conference on Learning Representations</em>.
</div>
<div id="ref-girdhar2023imagebind" class="csl-entry" role="listitem">
Girdhar, Rohit, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. <span>“ImageBind: One Embedding Space to Bind Them All.”</span> <em>arXiv Preprint arXiv:2305.05665</em>.
</div>
<div id="ref-graves2014neural" class="csl-entry" role="listitem">
Graves, Alex, Greg Wayne, and Ivo Danihelka. 2014. <span>“Neural Turing Machines.”</span> <em>arXiv Preprint arXiv:1410.5401</em>.
</div>
<div id="ref-he2021masked" class="csl-entry" role="listitem">
He, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2021. <span>“Masked Autoencoders Are Scalable Vision Learners.”</span> <em>arXiv Preprint arXiv:2111.06377</em>.
</div>
<div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 770–78.
</div>
<div id="ref-hoffmann2022training" class="csl-entry" role="listitem">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>“Training Compute-Optimal Large Language Models.”</span> <em>arXiv Preprint arXiv:2203.15556</em>.
</div>
<div id="ref-ioffe2015batch" class="csl-entry" role="listitem">
Ioffe, Sergey, and Christian Szegedy. 2015. <span>“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.”</span> In <em>International Conference on Machine Learning</em>, 448–56.
</div>
<div id="ref-le2022bloom" class="csl-entry" role="listitem">
Le Scao, Teven, Angela Fan, Christopher Akiki, Pavlick Ellie, Ilić Suzana, Hesslow Daniel, Castagné Roman, et al. 2022. <span>“BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.”</span> <em>arXiv Preprint arXiv:2211.05100</em>.
</div>
<div id="ref-lewis2019bart" class="csl-entry" role="listitem">
Lewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. <span>“BART: Denoising Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension.”</span> <em>arXiv Preprint arXiv:1910.13461</em>.
</div>
<div id="ref-lewkowycz2022solving" class="csl-entry" role="listitem">
Lewkowycz, Aitor, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, et al. 2022. <span>“Solving Quantitative Reasoning Problems with Language Models.”</span> <em>arXiv Preprint arXiv:2206.14858</em>.
</div>
<div id="ref-liu2023lost" class="csl-entry" role="listitem">
Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. <span>“Lost in the Middle: How Language Models Use Long Contexts.”</span> <em>arXiv Preprint arXiv:2307.03172</em>.
</div>
<div id="ref-liu2019roberta" class="csl-entry" role="listitem">
Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. <span>“RoBERTa: A Robustly Optimized BERT Pretraining Approach.”</span> <em>arXiv Preprint arXiv:1907.11692</em>.
</div>
<div id="ref-lu2022unified" class="csl-entry" role="listitem">
Lu, Jiasen, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. 2022. <span>“Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks.”</span> <em>arXiv Preprint arXiv:2206.08916</em>.
</div>
<div id="ref-luong2015effective" class="csl-entry" role="listitem">
Luong, Minh-Thang, Hieu Pham, and Christopher D Manning. 2015. <span>“Effective Approaches to Attention-Based Neural Machine Translation.”</span> <em>arXiv Preprint arXiv:1508.04025</em>.
</div>
<div id="ref-mehta2022mobilevit" class="csl-entry" role="listitem">
Mehta, Sachin, and Mohammad Rastegari. 2022. <span>“MobileViT: Light-Weight, General-Purpose, and Mobile-Friendly Vision Transformer.”</span> <em>arXiv Preprint arXiv:2110.02178</em>.
</div>
<div id="ref-openai2023gpt4" class="csl-entry" role="listitem">
OpenAI. 2023. <span>“GPT-4 Technical Report.”</span> <em>arXiv Preprint arXiv:2303.08774</em>.
</div>
<div id="ref-parmar2018image" class="csl-entry" role="listitem">
Parmar, Niki, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. <span>“Image Transformer.”</span> In <em>Proceedings of the 35th International Conference on Machine Learning</em>, 4055–64.
</div>
<div id="ref-radford2021learning" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning Transferable Visual Models from Natural Language Supervision.”</span> In <em>International Conference on Machine Learning</em>, 8748–63.
</div>
<div id="ref-radford2022robust" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. <span>“Robust Speech Recognition via Large-Scale Weak Supervision.”</span> <em>arXiv Preprint arXiv:2212.04356</em>.
</div>
<div id="ref-radford2018improving" class="csl-entry" role="listitem">
Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. <span>“Improving Language Understanding by Generative Pre-Training.”</span>
</div>
<div id="ref-radford2019language" class="csl-entry" role="listitem">
Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. <span>“Language Models Are Unsupervised Multitask Learners.”</span> <em>OpenAI Blog</em> 1 (8).
</div>
<div id="ref-raffel2019exploring" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. <span>“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.”</span> <em>arXiv Preprint arXiv:1910.10683</em>.
</div>
<div id="ref-ramesh2022hierarchical" class="csl-entry" role="listitem">
Ramesh, Aditya, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. <span>“Hierarchical Text-Conditional Image Generation with CLIP Latents.”</span> <em>arXiv Preprint arXiv:2204.06125</em>.
</div>
<div id="ref-reed2022generalist" class="csl-entry" role="listitem">
Reed, Scott, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, et al. 2022. <span>“A Generalist Agent.”</span> <em>arXiv Preprint arXiv:2205.06175</em>.
</div>
<div id="ref-rombach2022high" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. <span>“High-Resolution Image Synthesis with Latent Diffusion Models.”</span> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 10684–95.
</div>
<div id="ref-singhal2022large" class="csl-entry" role="listitem">
Singhal, Karan, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2022. <span>“Large Language Models Encode Clinical Knowledge.”</span> <em>arXiv Preprint arXiv:2212.13138</em>.
</div>
<div id="ref-srivastava2014dropout" class="csl-entry" role="listitem">
Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <span>“Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”</span> <em>Journal of Machine Learning Research</em> 15 (56): 1929–58.
</div>
<div id="ref-steiner2021train" class="csl-entry" role="listitem">
Steiner, Andreas, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. 2021. <span>“How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers.”</span> <em>arXiv Preprint arXiv:2106.10270</em>.
</div>
<div id="ref-sun2017revisiting" class="csl-entry" role="listitem">
Sun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. 2017. <span>“Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.”</span> In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 843–52.
</div>
<div id="ref-sun-etal-2021-long" class="csl-entry" role="listitem">
Sun, Simeng, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021. <span>“Do Long-Range Language Models Actually Use Long-Range Context?”</span> In <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 807–22. Online; Punta Cana, Dominican Republic: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2021.emnlp-main.62">https://doi.org/10.18653/v1/2021.emnlp-main.62</a>.
</div>
<div id="ref-tay2020efficient" class="csl-entry" role="listitem">
Tay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. <span>“Efficient Transformers: A Survey.”</span> <em>arXiv Preprint arXiv:2009.06732</em>.
</div>
<div id="ref-tay2022ul2" class="csl-entry" role="listitem">
Tay, Yi, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, et al. 2022. <span>“UL2: Unifying Language Learning Paradigms.”</span> <em>arXiv Preprint arXiv:2205.05131</em>.
</div>
<div id="ref-taylor2022galactica" class="csl-entry" role="listitem">
Taylor, Ross, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. <span>“Galactica: A Large Language Model for Science.”</span> <em>arXiv Preprint arXiv:2211.09085</em>.
</div>
<div id="ref-touvron2021augmenting" class="csl-entry" role="listitem">
Touvron, Hugo, Matthieu Cord, Alaaeldin El-Nouby, Piotr Bojanowski, Armand Joulin, Gabriel Synnaeve, and Hervé Jégou. 2021. <span>“Augmenting Convolutional Networks with Attention-Based Aggregation.”</span> <em>arXiv Preprint arXiv:2112.13692</em>.
</div>
<div id="ref-touvron2023llama" class="csl-entry" role="listitem">
Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. 2023. <span>“LLaMA: Open and Efficient Foundation Language Models.”</span> <em>arXiv Preprint arXiv:2302.13971</em>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>arXiv Preprint arXiv:1706.03762</em>.
</div>
<div id="ref-vig2019multiscale" class="csl-entry" role="listitem">
Vig, Jesse. 2019. <span>“A Multiscale Visualization of Attention in the Transformer Model.”</span> In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>, 37–42.
</div>
<div id="ref-wang2023clinicalgpt" class="csl-entry" role="listitem">
Wang, Guangyu, Guoxing Yang, Zongxin Du, Longjun Fan, and Xiaohu Li. 2023. <span>“ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation.”</span> <em>arXiv Preprint arXiv:2306.09968</em>.
</div>
<div id="ref-wang2022ofa" class="csl-entry" role="listitem">
Wang, Peng, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. <span>“OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework.”</span> In <em>Proceedings of the 39th International Conference on Machine Learning</em>, 23318–40. PMLR.
</div>
<div id="ref-wang2021simvlm" class="csl-entry" role="listitem">
Wang, Zirui, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2021. <span>“SimVLM: Simple Visual Language Model Pretraining with Weak Supervision.”</span> <em>arXiv Preprint arXiv:2108.10904</em>.
</div>
<div id="ref-wei2022chain" class="csl-entry" role="listitem">
Wei, Jason, Max Nye, and Percy Liang. 2022. <span>“Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.”</span> <em>arXiv Preprint arXiv:2201.11903</em>.
</div>
<div id="ref-wei2022emergent" class="csl-entry" role="listitem">
Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, et al. 2022. <span>“Emergent Abilities of Large Language Models.”</span> <em>arXiv Preprint arXiv:2206.07682</em>.
</div>
<div id="ref-wei2023larger" class="csl-entry" role="listitem">
Wei, Jerry, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, et al. 2023. <span>“Larger Language Models Do in-Context Learning Differently.”</span> <em>arXiv Preprint arXiv:2303.03846</em>.
</div>
<div id="ref-wolf-etal-2020-transformers" class="csl-entry" role="listitem">
Wolf, Thomas, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, et al. 2020. <span>“Transformers: State-of-the-Art Natural Language Processing.”</span> In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, 38–45. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-demos.6">https://doi.org/10.18653/v1/2020.emnlp-demos.6</a>.
</div>
<div id="ref-wu2023bloomberggpt" class="csl-entry" role="listitem">
Wu, Shijie, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. <span>“BloombergGPT: A Large Language Model for Finance.”</span> <em>arXiv Preprint arXiv:2303.17564</em>.
</div>
<div id="ref-xie2023residual" class="csl-entry" role="listitem">
Xie, Shufang, Huishuai Zhang, Junliang Guo, Xu Tan, Jiang Bian, Hany Hassan Awadalla, Arul Menezes, Tao Qin, and Rui Yan. 2023. <span>“ResiDual: Transformer with Dual Residual Connections.”</span> <em>arXiv Preprint arXiv:2304.14802</em>.
</div>
<div id="ref-xiong2020layer" class="csl-entry" role="listitem">
Xiong, Ruibin, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. <span>“On Layer Normalization in the Transformer Architecture.”</span> In <em>International Conference on Machine Learning</em>, 10524–33.
</div>
<div id="ref-xu2015show" class="csl-entry" role="listitem">
Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio. 2015. <span>“Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.”</span> In <em>International Conference on Machine Learning</em>, 2048–57.
</div>
<div id="ref-xue2020mt5" class="csl-entry" role="listitem">
Xue, Linting, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. <span>“mT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer.”</span> In <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 483–98.
</div>
<div id="ref-yang2023fingpt" class="csl-entry" role="listitem">
Yang, Hongyang, Xiao-Yang Liu, and Christina Dan Wang. 2023. <span>“FinGPT: Open-Source Financial Large Language Models.”</span> <em>arXiv Preprint arXiv:2306.06031</em>.
</div>
<div id="ref-yang2023harnessing" class="csl-entry" role="listitem">
Yang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. <span>“Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.”</span> <em>arXiv Preprint arXiv:2304.13712</em>.
</div>
<div id="ref-zhang2015text" class="csl-entry" role="listitem">
Zhang, Xiang, and Yann LeCun. 2015. <span>“Text Understanding from Scratch.”</span> <em>arXiv Preprint arXiv:1502.01710</em>.
</div>
<div id="ref-zhang2023meta" class="csl-entry" role="listitem">
Zhang, Yiyuan, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. 2023. <span>“Meta-Transformer: A Unified Framework for Multimodal Learning.”</span> <em>arXiv Preprint arXiv:2307.10802</em>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Example adapted from Deep Learning with Python by Francois Chollet<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In the transformer paper, MLPs are what referred to as feed-forward networks(FFNs). I find the terminology of FFNs confusing sometime. MLPs are feed-forward networks but not the other way around.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>If you want to see how embeddings look like and how words with same semantic meaning tend to be closer to each other, you can play with <a href="http://projector.tensorflow.org/">Embedding Projector</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The core operation in attention is the dot product between query and keys, which, being a summation operation, is permutation invariant<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Hat tip to Sebastian Raschka for sharing this in his <a href="https://magazine.sebastianraschka.com/">newsletter</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>BertViz be accessed at <a href="https://github.com/jessevig/bertviz">https://github.com/jessevig/bertviz</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Karpathy said that in a Twitter thread. Available here: <a href="https://twitter.com/karpathy/status/1655994367033884672?s=20">https://twitter.com/karpathy/status/1655994367033884672?s=20</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p> Next sentence prediction in BERT and next token prediction in standard transformer are different. The idea is roughly similar, but the former is usually for discriminative modelling while the later is for auto-regressive generative modelling<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Next token prediction in decoder LLMs is different to next sentence prediction in BERT. The former operates on token level while the later operates on sentence level<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>It’s fair to say that GPT-3 popularized prompt engineering.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>The inductive biases in ConvNets are the results of their translation invariance. Convolution itself is translation equivariance(changing the position of pixels changes the output) but pooling which is often used after convolution is translation invariant(changing the position of pixels doesn’t change the output) and this make the overall ConvNets translation invariant architecture<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>GPU main memory is called HBM which stands for High Bandwidth Memory<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Available at <a href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Available at <a href="https://github.com/google/trax">https://github.com/google/trax</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{nyandwi2023,
  author = {Nyandwi, Jean},
  title = {The {Transformer} {Blueprint:} {A} {Holistic} {Guide} to the
    {Transformer} {Neural} {Network} {Architecture}},
  journal = {Deep Learning Revision},
  date = {2023-07-29},
  url = {https://deeprevision.github.io/posts/001-transformer/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-nyandwi2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Nyandwi, Jean. 2023. <span>“The Transformer Blueprint: A Holistic Guide
to the Transformer Neural Network Architecture.”</span> <em>Deep
Learning Revision</em>, July. <a href="https://deeprevision.github.io/posts/001-transformer/">https://deeprevision.github.io/posts/001-transformer/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>